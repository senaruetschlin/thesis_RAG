{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64bc71e7",
   "metadata": {},
   "source": [
    "# Create the knowledge graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb356a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/Users/christel/Desktop/Thesis/thesis_repo/data/data_processed/Keep/embedded_chunks.json\", 'r', encoding='utf-8') as file:\n",
    "    embedded_chunks = json.load(file)\n",
    "\n",
    "print(f\"Loaded {len(embedded_chunks)} embedded chunks\")\n",
    "if embedded_chunks:\n",
    "    last_sample = embedded_chunks[-1]\n",
    "    print(\"Last sample in embedded_chunks:\")\n",
    "    print(json.dumps(last_sample, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02572339",
   "metadata": {},
   "source": [
    "Test run: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b309efbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase\n",
    "import os\n",
    "\n",
    "def get_driver():\n",
    "    uri = os.getenv(\"NEO4J_URI\")\n",
    "    user = os.getenv(\"NEO4J_USERNAME\")\n",
    "    pwd = os.getenv(\"NEO4J_PASSWORD\")\n",
    "\n",
    "    if not all([uri, user, pwd]):\n",
    "        raise RuntimeError(\"Missing Neo4j connection info in environment variables\")\n",
    "\n",
    "    return GraphDatabase.driver(uri, auth=(user, pwd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f117bdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_chunk = next(c for c in embedded_chunks if c[\"chunk_id\"] == \"row_5823_chunk_0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dbce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6d8342",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import signal\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_neo4j import Neo4jGraph\n",
    "from langchain_community.graphs.graph_document import Node, Relationship\n",
    "\n",
    "# === Setup Logging ===\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(message)s\")\n",
    "\n",
    "# === Load Environment Variables ===\n",
    "load_dotenv()\n",
    "\n",
    "# === Config ===\n",
    "EMBEDDINGS_JSON  = \"/Users/christel/Desktop/Thesis/thesis_repo/data/data_processed/embedded_chunks.json\"\n",
    "CHUNK_VECTOR_PROPERTY = \"embedding\"\n",
    "TIMEOUT_SECONDS  = 300\n",
    "\n",
    "# === Init LLM ===\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "def init_llm():\n",
    "    return ChatOpenAI(\n",
    "        openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        model_name=\"gpt-4o-mini\"\n",
    "    )\n",
    "\n",
    "llm = init_llm()\n",
    "\n",
    "# === Init Neo4j client ===\n",
    "graph = Neo4jGraph(\n",
    "    url=os.getenv(\"NEO4J_URI\"),\n",
    "    username=os.getenv(\"NEO4J_USERNAME\"),\n",
    "    password=os.getenv(\"NEO4J_PASSWORD\")\n",
    ")\n",
    "\n",
    "# === Timeout Utilities ===\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutException()\n",
    "\n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "\n",
    "graph.query(\"CREATE CONSTRAINT unique_document_id IF NOT EXISTS FOR (d:Document) REQUIRE d.id IS UNIQUE\")\n",
    "graph.query(\"CREATE CONSTRAINT unique_chunk_id    IF NOT EXISTS FOR (c:Chunk)    REQUIRE c.id IS UNIQUE\")\n",
    "graph.query(\"CREATE CONSTRAINT unique_entity_id   IF NOT EXISTS FOR (e:Entity)   REQUIRE e.id IS UNIQUE\")\n",
    "\n",
    "# === Load precomputed embeddings ===\n",
    "with open(EMBEDDINGS_JSON, \"r\") as f:\n",
    "    embedded_chunks = json.load(f)\n",
    "\n",
    "# === test sample ===\n",
    "test_chunk = next(c for c in embedded_chunks if c[\"chunk_id\"] == \"row_5823_chunk_0\")\n",
    "chunk_id   = test_chunk[\"chunk_id\"]     \n",
    "row_index  = test_chunk[\"row_index\"]    \n",
    "text       = test_chunk[\"text\"]\n",
    "embedding  = test_chunk[\"embedding\"]\n",
    "\n",
    "logging.info(f\"Inserting single chunk: {chunk_id} (row_index={row_index})\")\n",
    "\n",
    "# --- Step 1: MERGE Document, Chunk and set properties + vector ---\n",
    "signal.alarm(TIMEOUT_SECONDS)\n",
    "try:\n",
    "    graph.query(\n",
    "        \"\"\"\n",
    "        MERGE (d:Document {id: $doc_id})\n",
    "        MERGE (c:Chunk    {id: $chunk_id})\n",
    "        SET d.row_index = $row_index,\n",
    "            c.text       = $text\n",
    "        MERGE (c)-[:PART_OF]->(d)\n",
    "        WITH c\n",
    "        CALL db.create.setNodeVectorProperty(c, $vector_prop, $vector)\n",
    "        \"\"\",\n",
    "        {\n",
    "            \"doc_id\":      str(row_index),\n",
    "            \"chunk_id\":    chunk_id,\n",
    "            \"row_index\":   row_index,\n",
    "            \"text\":        text,\n",
    "            \"vector_prop\": CHUNK_VECTOR_PROPERTY,\n",
    "            \"vector\":      embedding,\n",
    "        }\n",
    "    )\n",
    "    signal.alarm(0)\n",
    "    logging.info(\"Document & Chunk inserted (with embedding).\")\n",
    "except TimeoutException:\n",
    "    logging.error(\"Timeout during chunk/document insertion.\")\n",
    "    raise\n",
    "\n",
    "\n",
    "# --- Step 2: Extract entities from the chunk text ---\n",
    "doc_transformer = LLMGraphTransformer(llm=llm)\n",
    "\n",
    "signal.alarm(TIMEOUT_SECONDS)\n",
    "try:\n",
    "    fake_doc   = Document(page_content=text, metadata={\"row_index\": row_index})\n",
    "    graph_docs = doc_transformer.convert_to_graph_documents([fake_doc])\n",
    "    signal.alarm(0)\n",
    "    if not graph_docs or not graph_docs[0].nodes:\n",
    "        logging.warning(\"No entities extracted.\")\n",
    "except TimeoutException:\n",
    "    logging.error(\"Timeout during entity extraction.\")\n",
    "    raise\n",
    "\n",
    "# --- Step 3: Normalize the extracted nodes + build HAS_ENTITY rels ---\n",
    "for graph_doc in graph_docs:\n",
    "    entities = []\n",
    "    relationships = []\n",
    "\n",
    "    # Normalize each LLM‐extracted node\n",
    "    for n in graph_doc.nodes:\n",
    "        raw_id   = (n.id or \"\").strip()\n",
    "        raw_type = (n.type or \"\").strip()\n",
    "        if not raw_id or not raw_type:\n",
    "            logging.warning(f\"Skipping malformed: id={raw_id}, type={raw_type}\")\n",
    "            continue\n",
    "\n",
    "        eid = raw_id.lower()\n",
    "        n.id   = eid\n",
    "        n.type = \"Entity\"\n",
    "        n.properties[\"id\"]            = eid\n",
    "        n.properties[\"original_type\"] = raw_type\n",
    "\n",
    "        entities.append(n)\n",
    "        # build a rel from existing chunk to this entity\n",
    "        relationships.append(\n",
    "            Relationship(\n",
    "                source=Node(id=chunk_id, type=\"Chunk\"), \n",
    "                target=n, \n",
    "                type=\"HAS_ENTITY\"\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # overwrite the graph_doc\n",
    "    graph_doc.nodes = entities\n",
    "    graph_doc.relationships = relationships\n",
    "\n",
    "\n",
    "# --- Step 4: Push entities + HAS_ENTITY → chunk\n",
    "signal.alarm(TIMEOUT_SECONDS)\n",
    "try:\n",
    "    graph.add_graph_documents(\n",
    "        graph_docs,\n",
    "        baseEntityLabel=\"Entity\",   # all nodes in graph_doc.nodes get :Entity\n",
    "        include_source=False\n",
    "    )\n",
    "    signal.alarm(0)\n",
    "    logging.info(\"Entities + relationships inserted.\")\n",
    "except TimeoutException:\n",
    "    logging.error(\"Timeout during graph insertion.\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb62dc8",
   "metadata": {},
   "source": [
    "All files: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7469c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import signal\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_experimental.graph_transformers import LLMGraphTransformer\n",
    "from langchain_neo4j import Neo4jGraph\n",
    "from langchain_community.graphs.graph_document import Node, Relationship\n",
    "\n",
    "# === Setup Logging ===\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(message)s\")\n",
    "\n",
    "# === Load Environment Variables ===\n",
    "load_dotenv()\n",
    "\n",
    "# === Config ===\n",
    "EMBEDDINGS_JSON        = \"/Users/christel/Desktop/Thesis/thesis_repo/data/data_processed/embedded_chunks.json\"\n",
    "CHUNK_VECTOR_PROPERTY  = \"embedding\"\n",
    "TIMEOUT_SECONDS        = 300\n",
    "START_FROM_ROW         = 5823   \n",
    "\n",
    "# === Init LLM with retries ===\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "def init_llm():\n",
    "    return ChatOpenAI(\n",
    "        openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        model_name=\"gpt-4o-mini\"\n",
    "    )\n",
    "\n",
    "llm = init_llm()\n",
    "\n",
    "# === Init Neo4j client ===\n",
    "graph = Neo4jGraph(\n",
    "    url=os.getenv(\"NEO4J_URI\"),\n",
    "    username=os.getenv(\"NEO4J_USERNAME\"),\n",
    "    password=os.getenv(\"NEO4J_PASSWORD\")\n",
    ")\n",
    "\n",
    "# === Timeout Utilities ===\n",
    "class TimeoutException(Exception):\n",
    "    pass\n",
    "\n",
    "def timeout_handler(signum, frame):\n",
    "    raise TimeoutException()\n",
    "\n",
    "signal.signal(signal.SIGALRM, timeout_handler)\n",
    "\n",
    "# === Ensure uniqueness constraints ===\n",
    "graph.query(\"CREATE CONSTRAINT unique_document_id IF NOT EXISTS FOR (d:Document) REQUIRE d.id IS UNIQUE\")\n",
    "graph.query(\"CREATE CONSTRAINT unique_chunk_id    IF NOT EXISTS FOR (c:Chunk)    REQUIRE c.id IS UNIQUE\")\n",
    "graph.query(\"CREATE CONSTRAINT unique_entity_id   IF NOT EXISTS FOR (e:Entity)   REQUIRE e.id IS UNIQUE\")\n",
    "\n",
    "# === Load Precomputed Embeddings ===\n",
    "with open(EMBEDDINGS_JSON, \"r\") as f:\n",
    "    embedded_chunks = json.load(f)\n",
    "\n",
    "# === Prepare the LLM→Graph transformer ===\n",
    "doc_transformer = LLMGraphTransformer(llm=llm)\n",
    "\n",
    "# === Main Loop over all chunks ===\n",
    "for item in embedded_chunks:\n",
    "    chunk_id  = item[\"chunk_id\"]\n",
    "    row_index = item[\"row_index\"]\n",
    "    text      = item[\"text\"]\n",
    "    vector    = item[\"embedding\"]\n",
    "\n",
    "    # Skip until we reach our starting row\n",
    "    if row_index < START_FROM_ROW:\n",
    "        continue\n",
    "\n",
    "    logging.info(f\"Processing chunk {chunk_id} (row_index={row_index})\")\n",
    "\n",
    "    # --- Step 1: MERGE Document & Chunk + set text & embedding ---\n",
    "    signal.alarm(TIMEOUT_SECONDS)\n",
    "    try:\n",
    "        graph.query(\n",
    "            \"\"\"\n",
    "            MERGE (d:Document {id: $doc_id})\n",
    "            SET d.row_index = $row_index\n",
    "            MERGE (c:Chunk {id: $chunk_id})\n",
    "            SET c.text = $text\n",
    "            MERGE (c)-[:PART_OF]->(d)\n",
    "            WITH c\n",
    "            CALL db.create.setNodeVectorProperty(c, $vector_prop, $vector)\n",
    "            \"\"\",\n",
    "            {\n",
    "                \"doc_id\":      str(row_index),\n",
    "                \"row_index\":   row_index,\n",
    "                \"chunk_id\":    chunk_id,\n",
    "                \"text\":        text,\n",
    "                \"vector_prop\": CHUNK_VECTOR_PROPERTY,\n",
    "                \"vector\":      vector,\n",
    "            }\n",
    "        )\n",
    "        signal.alarm(0)\n",
    "        logging.info(\"Document & Chunk upserted (with embedding).\")\n",
    "    except TimeoutException:\n",
    "        logging.error(f\"Timeout inserting chunk {chunk_id}\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error inserting chunk {chunk_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # --- Step 2: Extract entities from the chunk text ---\n",
    "    signal.alarm(TIMEOUT_SECONDS)\n",
    "    try:\n",
    "        fake_doc   = Document(page_content=text, metadata={\"row_index\": row_index})\n",
    "        graph_docs = doc_transformer.convert_to_graph_documents([fake_doc])\n",
    "        signal.alarm(0)\n",
    "    except TimeoutException:\n",
    "        logging.error(f\"Timeout extracting entities for {chunk_id}\")\n",
    "        continue\n",
    "    except Exception as e:\n",
    "        logging.error(f\"LLM graph extraction failed for {chunk_id}: {e}\")\n",
    "        continue\n",
    "\n",
    "    if not graph_docs or not graph_docs[0].nodes:\n",
    "        logging.info(f\"No entities found for {chunk_id}\")\n",
    "        continue\n",
    "\n",
    "    # --- Step 3: Normalize entities + build relationships to the chunk ---\n",
    "    graph_doc = graph_docs[0]\n",
    "    entities     = []\n",
    "    relationships = []\n",
    "    chunk_node   = Node(id=chunk_id, type=\"Chunk\")\n",
    "\n",
    "    for n in graph_doc.nodes:\n",
    "        raw_id   = (n.id or \"\").strip()\n",
    "        raw_type = (n.type or \"\").strip()\n",
    "        if not raw_id or not raw_type:\n",
    "            logging.warning(f\"Skipping malformed entity: id={raw_id}, type={raw_type}\")\n",
    "            continue\n",
    "\n",
    "        eid = raw_id.lower()\n",
    "        n.id   = eid\n",
    "        n.type = \"Entity\"\n",
    "        n.properties[\"id\"]            = eid\n",
    "        n.properties[\"original_type\"] = raw_type\n",
    "\n",
    "        entities.append(n)\n",
    "        relationships.append(\n",
    "            Relationship(source=chunk_node, target=n, type=\"HAS_ENTITY\")\n",
    "        )\n",
    "\n",
    "    graph_doc.nodes = entities\n",
    "    graph_doc.relationships = relationships\n",
    "\n",
    "    # --- Step 4: Manually MERGE each Entity + HAS_ENTITY rel per chunk ---\n",
    "    signal.alarm(TIMEOUT_SECONDS)\n",
    "    try:\n",
    "        # upsert all entities for this chunk\n",
    "        for ent in entities:\n",
    "            graph.query(\n",
    "                \"\"\"\n",
    "                MERGE (e:Entity {id: $entity_id})\n",
    "                SET e.original_type = $original_type\n",
    "                \"\"\",\n",
    "                {\n",
    "                    \"entity_id\":    ent.id,\n",
    "                    \"original_type\": ent.properties[\"original_type\"],\n",
    "                },\n",
    "            )\n",
    "\n",
    "        # upsert the relationships from chunk→entity\n",
    "        for ent in entities:\n",
    "            graph.query(\n",
    "                \"\"\"\n",
    "                MATCH (c:Chunk {id: $chunk_id})\n",
    "                MATCH (e:Entity {id: $entity_id})\n",
    "                MERGE (c)-[:HAS_ENTITY]->(e)\n",
    "                \"\"\",\n",
    "                {\n",
    "                    \"chunk_id\":    chunk_id,\n",
    "                    \"entity_id\":   ent.id,\n",
    "                },\n",
    "            )\n",
    "\n",
    "        signal.alarm(0)\n",
    "        logging.info(f\"Entities & relationships inserted for {chunk_id}\")\n",
    "    except TimeoutException:\n",
    "        logging.error(f\"Timeout inserting entities for {chunk_id}\")\n",
    "        signal.alarm(0)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b7d704",
   "metadata": {},
   "source": [
    "# Implement the Retriever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd484890",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "#sys.path.append(\"/Users/christel/Desktop/Thesis/thesis_repo/src/retrievers/graphrag\")\n",
    "#/Users/christel/Desktop/Thesis/thesis_repo/src/retrievers/graphrag/graphRAG_retriever.py\n",
    "sys.path.append(\"/Users/christel/Desktop/Thesis/thesis_repo\")\n",
    "\n",
    "load_dotenv()\n",
    "from src.retrievers.hybridrag.graphRAG_retriever import retrieve\n",
    "\n",
    "query = \"For STT, what was the percent change in the value of commercial paper outstanding between 2010 and 2011?\"\n",
    "results = retrieve(query)\n",
    "\n",
    "print(\"=== Stage 1 Output ===\")\n",
    "print(results[\"stage1\"])\n",
    "\n",
    "print(\"\\n=== Top Retrieved Chunks ===\")\n",
    "for r in results[\"candidates\"]:\n",
    "    print(f\"Chunk ID: {r['chunkId']}\")\n",
    "    print(f\"Fused Score: {r['fusedScore']:.4f}\")\n",
    "    print(f\"Entities: {r['entities']}\")\n",
    "    print(f\"Text: {r['text'][:300]}...\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
