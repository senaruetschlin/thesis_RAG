{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dataset Exploration***\n",
    "In the following we will analyse the dataset used for our research. This allows proper pre-processing and a sound research design enabling meaningful insights. \n",
    "The primary aim is to understand the structure of each dataset, which allows us to unify them to construct the final dataset, and to analyze the data distribution and characteristics, which enables efficient sampling. \n",
    "\n",
    "The dataset used for our research are: FinQA, ConFinQA, and FinDER.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all relevant libraries\n",
    "import json\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. First Dataset Insepection**\n",
    "\n",
    "**Load and Inspect the Samples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "import json\n",
    "from pathlib import Path\n",
    "# For better display in notebooks\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FinQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6251 training examples\n",
      "Data type: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "finQA_train_file = Path(\"/Users/christel/Desktop/Thesis/thesis_repo/data/FinQA-main/dataset/train.json\")\n",
    "with open(finQA_train_file, 'r') as f:\n",
    "    finQA_train_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(finQA_train_data)} training examples\")\n",
    "print(f\"Data type: {type(finQA_train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample type: <class 'dict'>\n",
      "Sample keys: ['pre_text', 'post_text', 'filename', 'table_ori', 'table', 'qa', 'id', 'table_retrieved', 'text_retrieved', 'table_retrieved_all', 'text_retrieved_all']\n",
      "Number of keys: 11\n"
     ]
    }
   ],
   "source": [
    "# Inspect the structure of the first sample\n",
    "finQA_first_sample = finQA_train_data[0]\n",
    "print(f\"Sample type: {type(finQA_first_sample)}\")\n",
    "print(f\"Sample keys: {list(finQA_first_sample.keys())}\")\n",
    "print(f\"Number of keys: {len(finQA_first_sample.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pre_text': ['interest rate to a variable interest rate based on the three-month libor plus 2.05% ( 2.05 % ) ( 2.34% ( 2.34 % ) as of october 31 , 2009 ) .', 'if libor changes by 100 basis points , our annual interest expense would change by $ 3.8 million .', 'foreign currency exposure as more fully described in note 2i .', 'in the notes to consolidated financial statements contained in item 8 of this annual report on form 10-k , we regularly hedge our non-u.s .', 'dollar-based exposures by entering into forward foreign currency exchange contracts .', 'the terms of these contracts are for periods matching the duration of the underlying exposure and generally range from one month to twelve months .', 'currently , our largest foreign currency exposure is the euro , primarily because our european operations have the highest proportion of our local currency denominated expenses .', 'relative to foreign currency exposures existing at october 31 , 2009 and november 1 , 2008 , a 10% ( 10 % ) unfavorable movement in foreign currency exchange rates over the course of the year would not expose us to significant losses in earnings or cash flows because we hedge a high proportion of our year-end exposures against fluctuations in foreign currency exchange rates .', 'the market risk associated with our derivative instruments results from currency exchange rate or interest rate movements that are expected to offset the market risk of the underlying transactions , assets and liabilities being hedged .', 'the counterparties to the agreements relating to our foreign exchange instruments consist of a number of major international financial institutions with high credit ratings .', 'we do not believe that there is significant risk of nonperformance by these counterparties because we continually monitor the credit ratings of such counterparties .', 'while the contract or notional amounts of derivative financial instruments provide one measure of the volume of these transactions , they do not represent the amount of our exposure to credit risk .', 'the amounts potentially subject to credit risk ( arising from the possible inability of counterparties to meet the terms of their contracts ) are generally limited to the amounts , if any , by which the counterparties 2019 obligations under the contracts exceed our obligations to the counterparties .', 'the following table illustrates the effect that a 10% ( 10 % ) unfavorable or favorable movement in foreign currency exchange rates , relative to the u.s .', 'dollar , would have on the fair value of our forward exchange contracts as of october 31 , 2009 and november 1 , 2008: .'], 'post_text': ['fair value of forward exchange contracts after a 10% ( 10 % ) unfavorable movement in foreign currency exchange rates asset ( liability ) .', '.', '.', '.', '.', '.', '.', '.', '.', '$ 20132 $ ( 9457 ) fair value of forward exchange contracts after a 10% ( 10 % ) favorable movement in foreign currency exchange rates liability .', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '$ ( 6781 ) $ ( 38294 ) the calculation assumes that each exchange rate would change in the same direction relative to the u.s .', 'dollar .', 'in addition to the direct effects of changes in exchange rates , such changes typically affect the volume of sales or the foreign currency sales price as competitors 2019 products become more or less attractive .', 'our sensitivity analysis of the effects of changes in foreign currency exchange rates does not factor in a potential change in sales levels or local currency selling prices. .'], 'filename': 'ADI/2009/page_49.pdf', 'table_ori': [['', 'October 31, 2009', 'November 1, 2008'], ['Fair value of forward exchange contracts asset (liability)', '$6,427', '$(23,158)'], ['Fair value of forward exchange contracts after a 10% unfavorable movement in foreign currency exchange rates asset (liability)', '$20,132', '$(9,457)'], ['Fair value of forward exchange contracts after a 10% favorable movement in foreign currency exchange rates liability', '$(6,781)', '$(38,294)']], 'table': [['', 'october 31 2009', 'november 1 2008'], ['fair value of forward exchange contracts asset ( liability )', '$ 6427', '$ -23158 ( 23158 )'], ['fair value of forward exchange contracts after a 10% ( 10 % ) unfavorable movement in foreign currency exchange rates asset ( liability )', '$ 20132', '$ -9457 ( 9457 )'], ['fair value of forward exchange contracts after a 10% ( 10 % ) favorable movement in foreign currency exchange rates liability', '$ -6781 ( 6781 )', '$ -38294 ( 38294 )']], 'qa': {'question': 'what is the the interest expense in 2009?', 'answer': '380', 'explanation': '', 'ann_table_rows': [], 'ann_text_rows': [1], 'steps': [{'op': 'divide1-1', 'arg1': '100', 'arg2': '100', 'res': '1%'}, {'op': 'divide1-2', 'arg1': '3.8', 'arg2': '#0', 'res': '380'}], 'program': 'divide(100, 100), divide(3.8, #0)', 'gold_inds': {'text_1': 'if libor changes by 100 basis points , our annual interest expense would change by $ 3.8 million .'}, 'exe_ans': 3.8, 'tfidftopn': {'text_14': 'dollar , would have on the fair value of our forward exchange contracts as of october 31 , 2009 and november 1 , 2008: .', 'text_0': 'interest rate to a variable interest rate based on the three-month libor plus 2.05% ( 2.05 % ) ( 2.34% ( 2.34 % ) as of october 31 , 2009 ) .'}, 'program_re': 'divide(3.8, divide(100, 100))', 'model_input': [['text_0', 'interest rate to a variable interest rate based on the three-month libor plus 2.05% ( 2.05 % ) ( 2.34% ( 2.34 % ) as of october 31 , 2009 ) .'], ['text_1', 'if libor changes by 100 basis points , our annual interest expense would change by $ 3.8 million .'], ['text_14', 'dollar , would have on the fair value of our forward exchange contracts as of october 31 , 2009 and november 1 , 2008: .']]}, 'id': 'ADI/2009/page_49.pdf-1', 'table_retrieved': [{'score': -0.6207679510116577, 'ind': 'table_1'}, {'score': -0.8948984742164612, 'ind': 'table_2'}], 'text_retrieved': [{'score': 1.251369595527649, 'ind': 'text_1'}, {'score': 0.6589734554290771, 'ind': 'text_0'}, {'score': -0.1914736032485962, 'ind': 'text_14'}], 'table_retrieved_all': [{'score': -0.6207679510116577, 'ind': 'table_1'}, {'score': -0.8948984742164612, 'ind': 'table_2'}, {'score': -1.2129878997802734, 'ind': 'table_3'}, {'score': -2.9782934188842773, 'ind': 'table_0'}], 'text_retrieved_all': [{'score': 1.251369595527649, 'ind': 'text_1'}, {'score': 0.6589734554290771, 'ind': 'text_0'}, {'score': -0.1914736032485962, 'ind': 'text_14'}, {'score': -1.0945320129394531, 'ind': 'text_47'}, {'score': -1.4916260242462158, 'ind': 'text_24'}, {'score': -1.5615578889846802, 'ind': 'text_12'}, {'score': -1.572263479232788, 'ind': 'text_15'}, {'score': -1.6337369680404663, 'ind': 'text_5'}, {'score': -1.678298830986023, 'ind': 'text_3'}, {'score': -1.6905218362808228, 'ind': 'text_6'}, {'score': -1.9114893674850464, 'ind': 'text_46'}, {'score': -1.914547324180603, 'ind': 'text_7'}, {'score': -1.955027461051941, 'ind': 'text_8'}, {'score': -2.0304598808288574, 'ind': 'text_49'}, {'score': -2.0383174419403076, 'ind': 'text_13'}, {'score': -2.112241268157959, 'ind': 'text_10'}, {'score': -2.1439552307128906, 'ind': 'text_11'}, {'score': -2.2258567810058594, 'ind': 'text_4'}, {'score': -2.409395694732666, 'ind': 'text_48'}, {'score': -2.6092159748077393, 'ind': 'text_9'}, {'score': -2.6313436031341553, 'ind': 'text_2'}, {'score': -2.932347536087036, 'ind': 'text_16'}, {'score': -2.932347536087036, 'ind': 'text_17'}, {'score': -2.932347536087036, 'ind': 'text_18'}, {'score': -2.932347536087036, 'ind': 'text_19'}, {'score': -2.932347536087036, 'ind': 'text_20'}, {'score': -2.932347536087036, 'ind': 'text_21'}, {'score': -2.932347536087036, 'ind': 'text_22'}, {'score': -2.932347536087036, 'ind': 'text_23'}, {'score': -2.932347536087036, 'ind': 'text_25'}, {'score': -2.932347536087036, 'ind': 'text_26'}, {'score': -2.932347536087036, 'ind': 'text_27'}, {'score': -2.932347536087036, 'ind': 'text_28'}, {'score': -2.932347536087036, 'ind': 'text_29'}, {'score': -2.932347536087036, 'ind': 'text_30'}, {'score': -2.932347536087036, 'ind': 'text_31'}, {'score': -2.932347536087036, 'ind': 'text_32'}, {'score': -2.932347536087036, 'ind': 'text_33'}, {'score': -2.932347536087036, 'ind': 'text_34'}, {'score': -2.932347536087036, 'ind': 'text_35'}, {'score': -2.932347536087036, 'ind': 'text_36'}, {'score': -2.932347536087036, 'ind': 'text_37'}, {'score': -2.932347536087036, 'ind': 'text_38'}, {'score': -2.932347536087036, 'ind': 'text_39'}, {'score': -2.932347536087036, 'ind': 'text_40'}, {'score': -2.932347536087036, 'ind': 'text_41'}, {'score': -2.932347536087036, 'ind': 'text_42'}, {'score': -2.932347536087036, 'ind': 'text_43'}, {'score': -2.932347536087036, 'ind': 'text_44'}, {'score': -2.932347536087036, 'ind': 'text_45'}]}\n"
     ]
    }
   ],
   "source": [
    "print(finQA_first_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‹ pre_text:\n",
      "   Type: list (length: 15)\n",
      "   First item type: <class 'str'>\n",
      "\n",
      "ðŸ“‹ post_text:\n",
      "   Type: list (length: 35)\n",
      "   First item type: <class 'str'>\n",
      "\n",
      "ðŸ“‹ filename:\n",
      "   Type: string (length: 20)\n",
      "   Preview: ADI/2009/page_49.pdf\n",
      "\n",
      "ðŸ“‹ table_ori:\n",
      "   Type: list (length: 4)\n",
      "   First item type: <class 'list'>\n",
      "\n",
      "ðŸ“‹ table:\n",
      "   Type: list (length: 4)\n",
      "   First item type: <class 'list'>\n",
      "\n",
      "ðŸ“‹ qa:\n",
      "   Type: dict (keys: ['question', 'answer', 'explanation', 'ann_table_rows', 'ann_text_rows', 'steps', 'program', 'gold_inds', 'exe_ans', 'tfidftopn', 'program_re', 'model_input'])\n",
      "\n",
      "ðŸ“‹ id:\n",
      "   Type: string (length: 22)\n",
      "   Preview: ADI/2009/page_49.pdf-1\n",
      "\n",
      "ðŸ“‹ table_retrieved:\n",
      "   Type: list (length: 2)\n",
      "   First item type: <class 'dict'>\n",
      "   First item keys: ['score', 'ind']\n",
      "\n",
      "ðŸ“‹ text_retrieved:\n",
      "   Type: list (length: 3)\n",
      "   First item type: <class 'dict'>\n",
      "   First item keys: ['score', 'ind']\n",
      "\n",
      "ðŸ“‹ table_retrieved_all:\n",
      "   Type: list (length: 4)\n",
      "   First item type: <class 'dict'>\n",
      "   First item keys: ['score', 'ind']\n",
      "\n",
      "ðŸ“‹ text_retrieved_all:\n",
      "   Type: list (length: 50)\n",
      "   First item type: <class 'dict'>\n",
      "   First item keys: ['score', 'ind']\n"
     ]
    }
   ],
   "source": [
    "# Detailed inspection of the first sample\n",
    "for key, value in finQA_first_sample.items():\n",
    "    print(f\"\\nðŸ“‹ {key}:\")\n",
    "    if isinstance(value, str):\n",
    "        print(f\"   Type: string (length: {len(value)})\")\n",
    "        print(f\"   Preview: {value[:100]}{'...' if len(value) > 100 else ''}\")\n",
    "    elif isinstance(value, list):\n",
    "        print(f\"   Type: list (length: {len(value)})\")\n",
    "        if len(value) > 0:\n",
    "            print(f\"   First item type: {type(value[0])}\")\n",
    "            if isinstance(value[0], dict):\n",
    "                print(f\"   First item keys: {list(value[0].keys())}\")\n",
    "    elif isinstance(value, dict):\n",
    "        print(f\"   Type: dict (keys: {list(value.keys())})\")\n",
    "    else:\n",
    "        print(f\"   Type: {type(value)}\")\n",
    "        print(f\"   Value: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FinQA: Each training example is a dictionary with 11 keys:** <br>\n",
    "\"pre_text\": the texts before the table; <br>\n",
    "\"post_text\": the text after the table;<br>\n",
    "\"filename\": name of the pdf file <br>\n",
    "\"table_ori\": The original version of the table, as extracted from the document, before any preprocessing or normalization.<br>\n",
    "\"table\": the table;<br>\n",
    "\"qa\": {<br>\n",
    "  \"question\": the question;<br>\n",
    "  \"answer\": The final numeric/textual answer to the question.<br>\n",
    "  \"explenation\": Optional human-written explanation for the answer (often empty in FinQA)<br>\n",
    "  \"ann_table_rows\": Indices of table rows that are annotated as relevant (if the answer comes from a table).<br>\n",
    "  \"ann_text_rows\": Indices of relevant text passages (e.g., [1] refers to text_1) from model_input.<br>\n",
    "  \"steps\" (\"op\": operation, \"arg1; arg2\": operands; \"res\": result of the operation:  The symbolic execution steps used to compute the answer.<br>\n",
    "  \"program\": the reasoning program;<br>\n",
    "  \"gold_inds\": the gold supporting facts;<br>\n",
    "  \"exe_ans\": the gold execution result;<br>\n",
    "  \"tfidftopn\": Top-n retrieved text chunks using TF-IDF baseline.<br>\n",
    "  \"program_re\": the reasoning program in nested format;<br>\n",
    "  \"model_input\": A list of text chunks (tuples of text ID and content) used as input to the model.<br>\n",
    "}<br>\n",
    "\"id\": unique example id. <br>\n",
    "\"table_retrieved\": A list of tables retrieved by a retriever model (e.g., BM25, DPR), each with a similarity score and ind (identifier).<br>\n",
    "\"text_retrieved\": A list of retrieved text passages (usually from pre_text + post_text), sorted by similarity score.<br>\n",
    "\"table_retrieved_all\": A complete list of table candidates along with their retrieval scores.<br>\n",
    "\"text_retrieved_all\": All candidate text chunks (with scores), potentially from the whole document, ranked by relevance.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ConvFinQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LICENSE', 'code', 'README.md', 'data']\n"
     ]
    }
   ],
   "source": [
    "print(os.listdir(\"/Users/christel/Desktop/Thesis/thesis_repo/data/ConvFinQA-main\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 11104 training examples\n",
      "Data type: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "ConvfinQA_turn_train_file = Path(\"/Users/christel/Desktop/Thesis/thesis_repo/data/ConvFinQA-main/data/train_turn.json\")\n",
    "with open(ConvfinQA_turn_train_file, 'r') as f:\n",
    "    ConvfinQA_turn_train_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(ConvfinQA_turn_train_data)} training examples\")\n",
    "print(f\"Data type: {type(ConvfinQA_turn_train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3037 training examples\n",
      "Data type: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "ConvfinQA_train_file = Path(\"/Users/christel/Desktop/Thesis/thesis_repo/data/ConvFinQA-main/data/train.json\")\n",
    "with open(ConvfinQA_train_file, 'r') as f:\n",
    "    ConvfinQA_train_data = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(ConvfinQA_train_data)} training examples\")\n",
    "print(f\"Data type: {type(ConvfinQA_train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 0: Dialogue length = 4\n",
      "['what is the net cash from operating activities in 2009?', 'what about in 2008?', 'what is the difference?', 'what percentage change does this represent?']\n",
      "\n",
      "Sample 1: Dialogue length = 4\n",
      "['what is the net cash from operating activities in 2009?', 'what about in 2008?', 'what is the difference?', 'what percentage change does this represent?']\n",
      "\n",
      "Sample 2: Dialogue length = 4\n",
      "['what is the net cash from operating activities in 2009?', 'what about in 2008?', 'what is the difference?', 'what percentage change does this represent?']\n",
      "\n",
      "Sample 3: Dialogue length = 4\n",
      "['what is the net cash from operating activities in 2009?', 'what about in 2008?', 'what is the difference?', 'what percentage change does this represent?']\n",
      "\n",
      "Sample 4: Dialogue length = 4\n",
      "['what were revenues in 2008?', 'what were they in 2007?', 'what was the net change?', 'what is the percent change?']\n"
     ]
    }
   ],
   "source": [
    "for i, sample in enumerate(ConvfinQA_turn_train_data[:5]):\n",
    "    dialogue = sample.get(\"annotation\", {}).get(\"dialogue_break\", [])\n",
    "    print(f\"\\nSample {i}: Dialogue length = {len(dialogue)}\")\n",
    "    print(dialogue)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample 0: Dialogue length = 4\n",
      "['what is the net cash from operating activities in 2009?', 'what about in 2008?', 'what is the difference?', 'what percentage change does this represent?']\n",
      "\n",
      "Sample 1: Dialogue length = 4\n",
      "['what were revenues in 2008?', 'what were they in 2007?', 'what was the net change?', 'what is the percent change?']\n",
      "\n",
      "Sample 2: Dialogue length = 4\n",
      "['what was the total of net sales in 2001?', 'and what was that in 2000?', 'what was, then, the change in the total of net sales over the year?', 'and how much does this change represent in relation to that total in 2000, in percentage?']\n",
      "\n",
      "Sample 3: Dialogue length = 6\n",
      "['what was the change in the performance of the united parcel service inc . from 2004 to 2009?', 'and how much does this change represent in relation to that performance in 2004, in percentage?', 'what was the performance value of the s&p 500 index in 2009?', 'what was, then, the change in that performance from 2004 to 2009?', 'and how much does this change represent in relation to that performance in 2004, in percentage?', 'what is, then, the difference between the percent representation of the united parcel service inc . and the s&p 500 index?']\n",
      "\n",
      "Sample 4: Dialogue length = 7\n",
      "['what was the fluctuation of the performance price of the ups from 2004 to 2006?', 'and how much does this fluctuation represent in relation to that price in 2004?', 'and from this year to 2009, what was the fluctuation for that stock?', 'what is this fluctuation as a percentage of the 2004 price?', 'and for the s&p 500 index price, what was the fluctuation in those five years?', 'and what percentage does this fluctuation represent in relation to the 2004 price of this stock?', 'what is, then, the difference between the ups percentage and this s&p 500 index one, for this five year period?']\n"
     ]
    }
   ],
   "source": [
    "for i, sample in enumerate(ConvfinQA_train_data[:5]):\n",
    "    dialogue = sample.get(\"annotation\", {}).get(\"dialogue_break\", [])\n",
    "    print(f\"\\nSample {i}: Dialogue length = {len(dialogue)}\")\n",
    "    print(dialogue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**train.json (Conversation-Level Format):** <br>\n",
    "Each entry in this file represents a full multi-turn dialogue between a user and a system. It contains multiple interrelated QA pairs (dialogue_break) that often require the model to reason across dialogue history. This format is ideal for training and evaluating systems designed to handle conversational memory and context-aware reasoning.<br>\n",
    "**train_turn.json (Turn-Level Format):**<br>\n",
    "This version contains individual QA pairs, each treated as an independent training instance. While each turn includes metadata about the full dialogue (e.g., dialogue_break, turn_program), the structure is flattened to focus on single-turn question answering. It aligns closely with traditional QA datasets like FinQA and FinDER.<br>\n",
    "\n",
    "\n",
    "The evaluation requires a unified dataset format that:<br>\n",
    "\n",
    "- Ensures consistency across multiple QA datasets (FinQA, FinDER, ConvFinQA),\n",
    "- Supports scalable benchmarking without additional engineering overhead,\n",
    "- Enables clean input-output tracking across different RAG pipelines.<br>\n",
    "\n",
    "The turn-level format (train_turn.json) satisfies these requirements by providing structurally uniform, self-contained QA pairs that are directly comparable to FinQA and FinDER. This consistency allows for streamlined preprocessing, batching, and evaluation across all models and datasets.<br>\n",
    "\n",
    "Additionally, using the turn-level format avoids the added complexity of reconstructing dialogue context or implementing query-rewriting logicâ€”an important consideration given the limited timeline of the project.<br>\n",
    "\n",
    "To still account for conversational realism, a small subset of context-dependent examples from train.json may be used in a complementary analysis, providing qualitative insights into retriever performance under dialogue-aware conditions.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample type: <class 'dict'>\n",
      "Sample keys: ['pre_text', 'post_text', 'filename', 'table_ori', 'table', 'qa', 'id', 'annotation']\n",
      "Number of keys: 8\n"
     ]
    }
   ],
   "source": [
    "# Inspect the structure of the first sample\n",
    "ConvfinQA_first_sample = ConvfinQA_turn_train_data[0]\n",
    "print(f\"Sample type: {type(ConvfinQA_first_sample)}\")\n",
    "print(f\"Sample keys: {list(ConvfinQA_first_sample.keys())}\")\n",
    "print(f\"Number of keys: {len(ConvfinQA_first_sample.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pre_text': ['26 | 2009 annual report in fiscal 2008 , revenues in the credit union systems and services business segment increased 14% ( 14 % ) from fiscal 2007 .', 'all revenue components within the segment experienced growth during fiscal 2008 .', 'license revenue generated the largest dollar growth in revenue as episys ae , our flagship core processing system aimed at larger credit unions , experienced strong sales throughout the year .', 'support and service revenue , which is the largest component of total revenues for the credit union segment , experienced 34 percent growth in eft support and 10 percent growth in in-house support .', 'gross profit in this business segment increased $ 9344 in fiscal 2008 compared to fiscal 2007 , due primarily to the increase in license revenue , which carries the highest margins .', 'liquidity and capital resources we have historically generated positive cash flow from operations and have generally used funds generated from operations and short-term borrowings on our revolving credit facility to meet capital requirements .', 'we expect this trend to continue in the future .', 'the company 2019s cash and cash equivalents increased to $ 118251 at june 30 , 2009 from $ 65565 at june 30 , 2008 .', 'the following table summarizes net cash from operating activities in the statement of cash flows : 2009 2008 2007 .'], 'post_text': ['year ended june 30 , cash provided by operations increased $ 25587 to $ 206588 for the fiscal year ended june 30 , 2009 as compared to $ 181001 for the fiscal year ended june 30 , 2008 .', 'this increase is primarily attributable to a decrease in receivables compared to the same period a year ago of $ 21214 .', 'this decrease is largely the result of fiscal 2010 annual software maintenance billings being provided to customers earlier than in the prior year , which allowed more cash to be collected before the end of the fiscal year than in previous years .', 'further , we collected more cash overall related to revenues that will be recognized in subsequent periods in the current year than in fiscal 2008 .', 'cash used in investing activities for the fiscal year ended june 2009 was $ 59227 and includes $ 3027 in contingent consideration paid on prior years 2019 acquisitions .', 'cash used in investing activities for the fiscal year ended june 2008 was $ 102148 and includes payments for acquisitions of $ 48109 , plus $ 1215 in contingent consideration paid on prior years 2019 acquisitions .', 'capital expenditures for fiscal 2009 were $ 31562 compared to $ 31105 for fiscal 2008 .', 'cash used for software development in fiscal 2009 was $ 24684 compared to $ 23736 during the prior year .', 'net cash used in financing activities for the current fiscal year was $ 94675 and includes the repurchase of 3106 shares of our common stock for $ 58405 , the payment of dividends of $ 26903 and $ 13489 net repayment on our revolving credit facilities .', 'cash used in financing activities was partially offset by proceeds of $ 3773 from the exercise of stock options and the sale of common stock ( through the employee stock purchase plan ) and $ 348 excess tax benefits from stock option exercises .', 'during fiscal 2008 , net cash used in financing activities for the fiscal year was $ 101905 and includes the repurchase of 4200 shares of our common stock for $ 100996 , the payment of dividends of $ 24683 and $ 429 net repayment on our revolving credit facilities .', 'cash used in financing activities was partially offset by proceeds of $ 20394 from the exercise of stock options and the sale of common stock and $ 3809 excess tax benefits from stock option exercises .', 'beginning during fiscal 2008 , us financial markets and many of the largest us financial institutions have been shaken by negative developments in the home mortgage industry and the mortgage markets , and particularly the markets for subprime mortgage-backed securities .', 'since that time , these and other such developments have resulted in a broad , global economic downturn .', 'while we , as is the case with most companies , have experienced the effects of this downturn , we have not experienced any significant issues with our current collection efforts , and we believe that any future impact to our liquidity will be minimized by cash generated by recurring sources of revenue and due to our access to available lines of credit. .'], 'filename': 'JKHY/2009/page_28.pdf', 'table_ori': [['', 'Year ended June 30, 2009'], ['2008', '2007'], ['Net income', '$103,102', '$104,222', '$104,681'], ['Non-cash expenses', '74,397', '70,420', '56,348'], ['Change in receivables', '21,214', '(2,913)', '(28,853)'], ['Change in deferred revenue', '21,943', '5,100', '24,576'], ['Change in other assets and liabilities', '(14,068)', '4,172', '17,495'], ['Net cash from operating activities', '$206,588', '$181,001', '$174,247']], 'table': [['2008', 'year ended june 30 2009 2008', 'year ended june 30 2009 2008', 'year ended june 30 2009'], ['net income', '$ 103102', '$ 104222', '$ 104681'], ['non-cash expenses', '74397', '70420', '56348'], ['change in receivables', '21214', '-2913 ( 2913 )', '-28853 ( 28853 )'], ['change in deferred revenue', '21943', '5100', '24576'], ['change in other assets and liabilities', '-14068 ( 14068 )', '4172', '17495'], ['net cash from operating activities', '$ 206588', '$ 181001', '$ 174247']], 'qa': {'question': 'what was the percentage change in the net cash from operating activities from 2008 to 2009', 'answer': '14.1%', 'explanation': '', 'ann_table_rows': [6], 'ann_text_rows': [], 'steps': [{'op': 'minus2-1', 'arg1': '206588', 'arg2': '181001', 'res': '25587'}, {'op': 'divide2-2', 'arg1': '#0', 'arg2': '181001', 'res': '14.1%'}], 'program': 'subtract(206588, 181001), divide(#0, 181001)', 'gold_inds': {'table_6': '2008 the net cash from operating activities of year ended june 30 2009 2008 is $ 206588 ; the net cash from operating activities of year ended june 30 2009 2008 is $ 181001 ; the net cash from operating activities of year ended june 30 2009 is $ 174247 ;'}, 'exe_ans': 0.14136, 'program_re': 'divide(subtract(206588, 181001), 181001)'}, 'id': 'Single_JKHY/2009/page_28.pdf-3_0', 'annotation': {'amt_table': \"<table class='wikitable'><tr><td>1</td><td>2008</td><td>year ended june 30 2009 2008</td><td>year ended june 30 2009 2008</td><td>year ended june 30 2009</td></tr><tr><td>2</td><td>net income</td><td>$ 103102</td><td>$ 104222</td><td>$ 104681</td></tr><tr><td>3</td><td>non-cash expenses</td><td>74397</td><td>70420</td><td>56348</td></tr><tr><td>4</td><td>change in receivables</td><td>21214</td><td>-2913 ( 2913 )</td><td>-28853 ( 28853 )</td></tr><tr><td>5</td><td>change in deferred revenue</td><td>21943</td><td>5100</td><td>24576</td></tr><tr><td>6</td><td>change in other assets and liabilities</td><td>-14068 ( 14068 )</td><td>4172</td><td>17495</td></tr><tr><td>7</td><td>net cash from operating activities</td><td>$ 206588</td><td>$ 181001</td><td>$ 174247</td></tr></table>\", 'amt_pre_text': '26 | 2009 annual report in fiscal 2008 , revenues in the credit union systems and services business segment increased 14% ( 14 % ) from fiscal 2007 . all revenue components within the segment experienced growth during fiscal 2008 . license revenue generated the largest dollar growth in revenue as episys ae , our flagship core processing system aimed at larger credit unions , experienced strong sales throughout the year . support and service revenue , which is the largest component of total revenues for the credit union segment , experienced 34 percent growth in eft support and 10 percent growth in in-house support . gross profit in this business segment increased $ 9344 in fiscal 2008 compared to fiscal 2007 , due primarily to the increase in license revenue , which carries the highest margins . liquidity and capital resources we have historically generated positive cash flow from operations and have generally used funds generated from operations and short-term borrowings on our revolving credit facility to meet capital requirements . we expect this trend to continue in the future . the company 2019s cash and cash equivalents increased to $ 118251 at june 30 , 2009 from $ 65565 at june 30 , 2008 . the following table summarizes net cash from operating activities in the statement of cash flows : 2009 2008 2007 .', 'amt_post_text': 'year ended june 30 , cash provided by operations increased $ 25587 to $ 206588 for the fiscal year ended june 30 , 2009 as compared to $ 181001 for the fiscal year ended june 30 , 2008 . this increase is primarily attributable to a decrease in receivables compared to the same period a year ago of $ 21214 . this decrease is largely the result of fiscal 2010 annual software maintenance billings being provided to customers earlier than in the prior year , which allowed more cash to be collected before the end of the fiscal year than in previous years . further , we collected more cash overall related to revenues that will be recognized in subsequent periods in the current year than in fiscal 2008 . cash used in investing activities for the fiscal year ended june 2009 was $ 59227 and includes $ 3027 in contingent consideration paid on prior years 2019 acquisitions . cash used in investing activities for the fiscal year ended june 2008 was $ 102148 and includes payments for acquisitions of $ 48109 , plus $ 1215 in contingent consideration paid on prior years 2019 acquisitions . capital expenditures for fiscal 2009 were $ 31562 compared to $ 31105 for fiscal 2008 . cash used for software development in fiscal 2009 was $ 24684 compared to $ 23736 during the prior year . net cash used in financing activities for the current fiscal year was $ 94675 and includes the repurchase of 3106 shares of our common stock for $ 58405 , the payment of dividends of $ 26903 and $ 13489 net repayment on our revolving credit facilities . cash used in financing activities was partially offset by proceeds of $ 3773 from the exercise of stock options and the sale of common stock ( through the employee stock purchase plan ) and $ 348 excess tax benefits from stock option exercises . during fiscal 2008 , net cash used in financing activities for the fiscal year was $ 101905 and includes the repurchase of 4200 shares of our common stock for $ 100996 , the payment of dividends of $ 24683 and $ 429 net repayment on our revolving credit facilities . cash used in financing activities was partially offset by proceeds of $ 20394 from the exercise of stock options and the sale of common stock and $ 3809 excess tax benefits from stock option exercises . beginning during fiscal 2008 , us financial markets and many of the largest us financial institutions have been shaken by negative developments in the home mortgage industry and the mortgage markets , and particularly the markets for subprime mortgage-backed securities . since that time , these and other such developments have resulted in a broad , global economic downturn . while we , as is the case with most companies , have experienced the effects of this downturn , we have not experienced any significant issues with our current collection efforts , and we believe that any future impact to our liquidity will be minimized by cash generated by recurring sources of revenue and due to our access to available lines of credit. .', 'original_program': 'subtract(206588, 181001), divide(A0, 181001)', 'step_list': ['Ask for number 206588', 'Ask for number 181001', 'subtract(206588, 181001)', 'divide(A0, 181001)'], 'answer_list': ['206588', '181001', 'A0', 'A1'], 'dialogue_break': ['what is the net cash from operating activities in 2009?', 'what about in 2008?', 'what is the difference?', 'what percentage change does this represent?'], 'turn_program_ori': ['206588', '181001', 'subtract(206588, 181001)', 'subtract(206588, 181001), divide(#0, 181001)'], 'dialogue_break_ori': ['what is the net cash from operating activities in 2009?', 'what about in 2008?', 'what is the difference?', 'what percentage change does this represent?'], 'turn_program': ['206588', '181001', 'subtract(206588, 181001)', 'subtract(206588, 181001), divide(#0, 181001)'], 'qa_split': [0, 0, 0, 0], 'exe_ans_list': [206588.0, 181001.0, 25587.0, 0.14136], 'cur_program': '206588', 'cur_dial': ['what is the net cash from operating activities in 2009?'], 'exe_ans': 206588.0, 'cur_type': 'number_turn', 'turn_ind': 0, 'gold_ind': {'table_6': '2008 the net cash from operating activities of year ended june 30 2009 2008 is $ 206588 ; the net cash from operating activities of year ended june 30 2009 2008 is $ 181001 ; the net cash from operating activities of year ended june 30 2009 is $ 174247 ;'}}}\n"
     ]
    }
   ],
   "source": [
    "print(ConvfinQA_first_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‹ pre_text:\n",
      "   Type: list (length: 9)\n",
      "   First item type: <class 'str'>\n",
      "\n",
      "ðŸ“‹ post_text:\n",
      "   Type: list (length: 15)\n",
      "   First item type: <class 'str'>\n",
      "\n",
      "ðŸ“‹ filename:\n",
      "   Type: string (length: 21)\n",
      "   Preview: JKHY/2009/page_28.pdf\n",
      "\n",
      "ðŸ“‹ table_ori:\n",
      "   Type: list (length: 8)\n",
      "   First item type: <class 'list'>\n",
      "\n",
      "ðŸ“‹ table:\n",
      "   Type: list (length: 7)\n",
      "   First item type: <class 'list'>\n",
      "\n",
      "ðŸ“‹ qa:\n",
      "   Type: dict (keys: ['question', 'answer', 'explanation', 'ann_table_rows', 'ann_text_rows', 'steps', 'program', 'gold_inds', 'exe_ans', 'program_re'])\n",
      "\n",
      "ðŸ“‹ id:\n",
      "   Type: string (length: 32)\n",
      "   Preview: Single_JKHY/2009/page_28.pdf-3_0\n",
      "\n",
      "ðŸ“‹ annotation:\n",
      "   Type: dict (keys: ['amt_table', 'amt_pre_text', 'amt_post_text', 'original_program', 'step_list', 'answer_list', 'dialogue_break', 'turn_program_ori', 'dialogue_break_ori', 'turn_program', 'qa_split', 'exe_ans_list', 'cur_program', 'cur_dial', 'exe_ans', 'cur_type', 'turn_ind', 'gold_ind'])\n"
     ]
    }
   ],
   "source": [
    "# Detailed inspection of the first sample\n",
    "for key, value in ConvfinQA_first_sample.items():\n",
    "    print(f\"\\nðŸ“‹ {key}:\")\n",
    "    if isinstance(value, str):\n",
    "        print(f\"   Type: string (length: {len(value)})\")\n",
    "        print(f\"   Preview: {value[:100]}{'...' if len(value) > 100 else ''}\")\n",
    "    elif isinstance(value, list):\n",
    "        print(f\"   Type: list (length: {len(value)})\")\n",
    "        if len(value) > 0:\n",
    "            print(f\"   First item type: {type(value[0])}\")\n",
    "            if isinstance(value[0], dict):\n",
    "                print(f\"   First item keys: {list(value[0].keys())}\")\n",
    "    elif isinstance(value, dict):\n",
    "        print(f\"   Type: dict (keys: {list(value.keys())})\")\n",
    "    else:\n",
    "        print(f\"   Type: {type(value)}\")\n",
    "        print(f\"   Value: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ConvFinQA: Each training example is a dictionary with 8 keys:** <br>\n",
    "\"pre_text\": the texts before the table; <br>\n",
    "\"post_text\": the text after the table;<br>\n",
    "\"filename\": name of the pdf file <br>\n",
    "\"table_ori\": The original version of the table, as extracted from the document, before any preprocessing or normalization.<br>\n",
    "\"table\": the table;<br>\n",
    "\"qa\": {<br>\n",
    "  \"question\": the question;<br>\n",
    "  \"answer\": The final numeric/textual answer to the question.<br>\n",
    "  \"ann_table_rows\": Indices of table rows that are annotated as relevant (if the answer comes from a table).<br>\n",
    "  \"ann_text_rows\": Indices of relevant text passages (e.g., [1] refers to text_1) from model_input.<br>\n",
    "  \"steps\" (\"op\": operation, \"arg1; arg2\": operands; \"res\": result of the operation:  The symbolic execution steps used to compute the answer.<br>\n",
    "  \"program\": the reasoning program;<br>\n",
    "  \"gold_inds\": the gold supporting facts;<br>\n",
    "  \"exe_ans\": the execution results of each question turn. ;<br>\n",
    "}<br>\n",
    "\"id\": unique example id. <br>\n",
    "\"annotation\": {<br>\n",
    "  \"original_program\": original FinQA question;<br>\n",
    "  \"dialogue_break\": the conversation, as a list of question turns. <br>\n",
    "  \"turn_program\": the ground truth program for each question, corresponding to the list in \"dialogue_break\".<br>\n",
    "  \"cur_pogram\":Current program for this turn.<br>\n",
    "  \"cur_dial\":Current dialogue turn.<br>\n",
    "  \"gold_ind\": Highlighted content for evidence.<br>\n",
    "  \"turn_ind\": Index of this turn in the full dialogue.<br>\n",
    "  \"exe_ans_list\": the execution results of each question turn. <br>\n",
    "}<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FinDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5703 training examples\n",
      "Data type: <class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "finder_train_file = Path(\"/Users/christel/Desktop/Thesis/thesis_repo/data/FinDER/train.jsonl\")\n",
    "finder_train_data = []\n",
    "with open(finder_train_file, 'r') as f:\n",
    "    for line in f:\n",
    "        finder_train_data.append(json.loads(line))\n",
    "\n",
    "print(f\"Loaded {len(finder_train_data)} training examples\")\n",
    "print(f\"Data type: {type(finder_train_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample type: <class 'dict'>\n",
      "Sample keys: ['_id', 'text', 'reasoning', 'category', 'references', 'answer', 'type']\n",
      "Number of keys: 7\n"
     ]
    }
   ],
   "source": [
    "# Inspect the structure of the first sample\n",
    "finder_first_sample = finder_train_data[0]\n",
    "print(f\"Sample type: {type(finder_first_sample)}\")\n",
    "print(f\"Sample keys: {list(finder_first_sample.keys())}\")\n",
    "print(f\"Number of keys: {len(finder_first_sample.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 'b33fcee7', 'text': 'Delta in CBOE Data & Access Solutions rev from 2021-23.', 'reasoning': True, 'category': 'Financials', 'references': ['Cboe Global Markets, Inc. and Subsidiaries\\n\\nConsolidated Statements of Income\\n\\nYears ended December 31, 2023, 2022, and 2021\\n\\n(In millions, except per share data)\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n    \\n\\n2023\\n\\n    \\n\\n2022\\n\\n    \\n\\n2021\\n\\n \\n\\nRevenues:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nCash and spot markets\\n\\n\\n$\\n\\n1,445.1\\n\\n\\n$\\n\\n1,777.6\\n\\n\\n$\\n\\n1,660.5\\n\\n\\nData and access solutions\\n\\n\\n\\n539.2\\n\\n\\n\\n497.0\\n\\n\\n\\n427.7\\n\\n\\nDerivatives markets\\n\\n\\n \\n\\n1,789.2\\n\\n\\n \\n\\n1,683.9\\n\\n\\n \\n\\n1,406.6\\n\\n\\nTotal revenues\\n\\n\\n \\n\\n3,773.5\\n\\n\\n \\n\\n3,958.5\\n\\n\\n \\n\\n3,494.8\\n\\n\\nCost of revenues:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  Liquidity payments\\n\\n\\n \\n\\n1,385.8\\n\\n\\n \\n\\n1,670.2\\n\\n\\n \\n\\n1,650.7\\n\\n\\n  Routing and clearing\\n\\n\\n\\n79.1\\n\\n\\n\\n83.2\\n\\n\\n\\n87.8\\n\\n\\n  Section 31 fees\\n\\n\\n\\n185.7\\n\\n\\n\\n329.8\\n\\n\\n\\n179.6\\n\\n\\n  Royalty fees and other cost of revenues\\n\\n\\n \\n\\n204.9\\n\\n\\n \\n\\n133.6\\n\\n\\n \\n\\n100.6\\n\\n\\nTotal cost of revenues\\n\\n\\n \\n\\n1,855.5\\n\\n\\n \\n\\n2,216.8\\n\\n\\n \\n\\n2,018.7\\n\\n\\nRevenues less cost of revenues\\n\\n\\n \\n\\n1,918.0\\n\\n\\n \\n\\n1,741.7\\n\\n\\n \\n\\n1,476.1\\n\\n\\nOperating expenses:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  Compensation and benefits\\n\\n\\n \\n\\n425.8\\n\\n\\n \\n\\n363.0\\n\\n\\n \\n\\n288.5\\n\\n\\n  Depreciation and amortization\\n\\n\\n \\n\\n158.0\\n\\n\\n \\n\\n166.8\\n\\n\\n \\n\\n167.4\\n\\n\\n  Technology support services\\n\\n\\n \\n\\n99.7\\n\\n\\n \\n\\n77.7\\n\\n\\n \\n\\n66.7\\n\\n\\n  Professional fees and outside services\\n\\n\\n \\n\\n92.0\\n\\n\\n \\n\\n89.0\\n\\n\\n \\n\\n83.7\\n\\n\\n  Travel and promotional expenses\\n\\n\\n \\n\\n37.6\\n\\n\\n \\n\\n23.7\\n\\n\\n \\n\\n9.7\\n\\n\\n  Facilities costs\\n\\n\\n \\n\\n25.7\\n\\n\\n \\n\\n25.1\\n\\n\\n \\n\\n22.2\\n\\n\\n  Acquisition-related costs\\n\\n\\n \\n\\n7.4\\n\\n\\n \\n\\n19.9\\n\\n\\n \\n\\n15.6\\n\\n\\n  Goodwill impairment\\n\\n\\n\\nâ€”\\n\\n\\n\\n460.9\\n\\n\\n\\nâ€”\\n\\n\\n  Other expenses\\n\\n\\n\\n13.9\\n\\n\\n\\n26.0\\n\\n\\n\\n16.4\\n\\n\\nTotal operating expenses\\n\\n\\n \\n\\n860.1\\n\\n\\n \\n\\n1,252.1\\n\\n\\n \\n\\n670.2\\n\\n\\nOperating income\\n\\n\\n \\n\\n1,057.9\\n\\n\\n \\n\\n489.6\\n\\n\\n \\n\\n805.9\\n\\n\\nNon-operating (expenses) income:\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nInterest expense\\n\\n\\n \\n\\n(62.4)\\n\\n\\n\\n(60.0)\\n\\n\\n\\n(48.0)\\n\\n\\nInterest income\\n\\n\\n\\n12.0\\n\\n\\n\\n3.6\\n\\n\\n\\n0.6\\n\\n\\nEarnings in investments\\n\\n\\n\\n39.5\\n\\n\\n\\n7.2\\n\\n\\n\\n1.0\\n\\n\\nOther income (expense), net\\n\\n\\n \\n\\n0.6\\n\\n\\n \\n\\n(7.5)\\n\\n\\n \\n\\n(3.4)\\n\\n\\nIncome before income tax provision\\n\\n\\n \\n\\n1,047.6\\n\\n\\n \\n\\n432.9\\n\\n\\n \\n\\n756.1\\n\\n\\nIncome tax provision\\n\\n\\n \\n\\n286.2\\n\\n\\n \\n\\n197.9\\n\\n\\n \\n\\n227.1\\n\\n\\nNet income\\n\\n\\n\\n761.4\\n\\n\\n\\n235.0\\n\\n\\n\\n529.0\\n\\n\\nNet income allocated to participating securities\\n\\n\\n\\n(3.9)\\n\\n\\n\\n(0.9)\\n\\n\\n\\n(1.7)\\n\\n\\nNet income allocated to common stockholders\\n\\n\\n$\\n\\n757.5\\n\\n\\n$\\n\\n234.1\\n\\n\\n$\\n\\n527.3\\n\\n\\nBasic earnings per share\\n\\n\\n$\\n\\n7.16\\n\\n\\n$\\n\\n2.20\\n\\n\\n$\\n\\n4.93\\n\\n\\nDiluted earnings per share\\n\\n\\n$\\n\\n7.13\\n\\n\\n$\\n\\n2.19\\n\\n\\n$\\n\\n4.92\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nBasic weighted average shares outstanding\\n\\n\\n\\n105.8\\n\\n\\n\\n106.3\\n\\n\\n\\n107.0\\n\\n\\nDiluted weighted average shares outstanding\\n\\n\\n\\n106.2\\n\\n\\n\\n106.7\\n\\n\\n\\n107.2'], 'answer': 'The Data and Access Solutions revenue increased by $111.5 million from 2021 to 2023, calculated as 539.2 million minus 427.7 million.', 'type': 'Subtract'}\n"
     ]
    }
   ],
   "source": [
    "print(finder_train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‹ _id:\n",
      "   Type: string (length: 8)\n",
      "   Preview: b33fcee7\n",
      "\n",
      "ðŸ“‹ text:\n",
      "   Type: string (length: 55)\n",
      "   Preview: Delta in CBOE Data & Access Solutions rev from 2021-23.\n",
      "\n",
      "ðŸ“‹ reasoning:\n",
      "   Type: <class 'bool'>\n",
      "   Value: True\n",
      "\n",
      "ðŸ“‹ category:\n",
      "   Type: string (length: 10)\n",
      "   Preview: Financials\n",
      "\n",
      "ðŸ“‹ references:\n",
      "   Type: list (length: 1)\n",
      "   First item type: <class 'str'>\n",
      "\n",
      "ðŸ“‹ answer:\n",
      "   Type: string (length: 133)\n",
      "   Preview: The Data and Access Solutions revenue increased by $111.5 million from 2021 to 2023, calculated as 5...\n",
      "\n",
      "ðŸ“‹ type:\n",
      "   Type: string (length: 8)\n",
      "   Preview: Subtract\n"
     ]
    }
   ],
   "source": [
    "# Detailed inspection of the first sample\n",
    "for key, value in finder_first_sample.items():\n",
    "    print(f\"\\nðŸ“‹ {key}:\")\n",
    "    if isinstance(value, str):\n",
    "        print(f\"   Type: string (length: {len(value)})\")\n",
    "        print(f\"   Preview: {value[:100]}{'...' if len(value) > 100 else ''}\")\n",
    "    elif isinstance(value, list):\n",
    "        print(f\"   Type: list (length: {len(value)})\")\n",
    "        if len(value) > 0:\n",
    "            print(f\"   First item type: {type(value[0])}\")\n",
    "            if isinstance(value[0], dict):\n",
    "                print(f\"   First item keys: {list(value[0].keys())}\")\n",
    "    elif isinstance(value, dict):\n",
    "        print(f\"   Type: dict (keys: {list(value.keys())})\")\n",
    "    else:\n",
    "        print(f\"   Type: {type(value)}\")\n",
    "        print(f\"   Value: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FinDER: Each training example is a dictionary with 7 keys:** <br>\n",
    "\"id\": unique identifier.<br>\n",
    "\"text\": query that the model is expected to answer.<br>\n",
    "\"reasoning\": ndicates whether the question requires reasoning (e.g. logical inference, arithmetic operations) rather than simple lookup. true = reasoning required.<br>\n",
    "\"category\": The semantic category of the question (e.g., Financials, Company overview, Footnotes, etc.).<br>\n",
    "\"references\": The source text passages (e.g., extracted from tables or footnotes) that the model should consider when answering the question. <br>\n",
    "\"answer\": The reference answer that the model should produce.<br>\n",
    "\"types\": Indicates the type of reasoning required to arrive at the answer. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **2. Construct the final dataset**<br>\n",
    "Since the original datasets are of varying structures we will in the following create a canonical schema to avoid confounds and log identical signals across runs. It'll contain only the fields that are relevant for retrieval, answer checking , and analysis. The goal is to construct a dataset where each row can be fed straight into the each of the RAG models with no dataset-specific branches. <br>\n",
    "\n",
    "\n",
    "\n",
    "The final dataset has the following structure: <br>\n",
    "\n",
    "{\n",
    "  \"qid\"          : \"string\",     // dataset-prefix + original id <br>\n",
    "  \"dataset\"      : \"FinQA | ConvFinQA | FinDER\",<br>\n",
    "  \"question\"     : \"string\",<br>\n",
    "  \"answer\"       : \"string\",     // canonicalised (see Â§4)<br>\n",
    "  \"context_text\" : [\"string\"],   // list of text passages (sentences or 100-token chunks)<br>\n",
    "  \"context_table\": [[\"string\"]], // normalised table (may be [])<br>\n",
    "  \"reasoning\"    : true|false,   // FinDER field â†’ others: len(steps)>1<br>\n",
    "  \"reason_type\"  : \"string|null\",// FinDER.type or Conv/FinQA program tag<br>\n",
    "  \"gold_text_id\" : [\"string\"],   // evidence indices, empty if not provided<br>\n",
    "  \"gold_table_row\":[int],        // ^ <br>\n",
    "  \"meta\"         : { ... }       // any extra fields you still need<br>\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Normalize datasets**\n",
    "\n",
    "2.1.1 FinQA\n",
    "\n",
    "We sentence-split each pre_text/post_text, then concatenate adjacent sentences until the segment is â‰¤ 100 BPE tokens. This follows best practice in prior RAG work (Lewis 2020; Izacard 2021) and balances retrieval precision with embedding quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process finqa with the function in data_utils.py\n",
    "\n",
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "from data_utils import preprocess_finqa_dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_finqa_sample(sample, tokenizer=None, max_bpe_tokens=100):\n",
    "    # Set up tokenizer if not provided\n",
    "    if tokenizer is None:\n",
    "        tokenizer = tiktoken.get_encoding(\"cl100k_base\")  # OpenAI's default\n",
    "\n",
    "    # Helper: sentence split (simple, can be improved)\n",
    "    def sentence_split(text):\n",
    "        import re\n",
    "        # Split on period, question mark, exclamation, or newline\n",
    "        return [s.strip() for s in re.split(r'(?<=[.?!])\\s+|\\n', text) if s.strip()]\n",
    "\n",
    "    # Helper: concatenate sentences â‰¤ max_bpe_tokens\n",
    "    def segment_sentences(sentences):\n",
    "        segments = []\n",
    "        current = \"\"\n",
    "        for sent in sentences:\n",
    "            if not current:\n",
    "                current = sent\n",
    "            else:\n",
    "                # Try adding the next sentence\n",
    "                test = current + \" \" + sent\n",
    "                if len(tokenizer.encode(test)) <= max_bpe_tokens:\n",
    "                    current = test\n",
    "                else:\n",
    "                    segments.append(current)\n",
    "                    current = sent\n",
    "        if current:\n",
    "            segments.append(current)\n",
    "        return segments\n",
    "\n",
    "    # 1. qid\n",
    "    qid = \"FinQA_\" + str(sample[\"id\"])\n",
    "    # 2. dataset\n",
    "    dataset = \"FinQA\"\n",
    "    # 3. question\n",
    "    question = sample[\"qa\"][\"question\"]\n",
    "    # 4. answer\n",
    "    answer = sample[\"qa\"][\"answer\"]\n",
    "    # 5. context_text\n",
    "    pre_text = sample.get(\"pre_text\", [])\n",
    "    post_text = sample.get(\"post_text\", [])\n",
    "    \n",
    "    # Handle pre_text and post_text as lists of strings\n",
    "    if isinstance(pre_text, list):\n",
    "        pre_sentences = []\n",
    "        for text_chunk in pre_text:\n",
    "            if isinstance(text_chunk, str):\n",
    "                pre_sentences.extend(sentence_split(text_chunk))\n",
    "    else:\n",
    "        pre_sentences = sentence_split(pre_text) if isinstance(pre_text, str) else []\n",
    "    \n",
    "    if isinstance(post_text, list):\n",
    "        post_sentences = []\n",
    "        for text_chunk in post_text:\n",
    "            if isinstance(text_chunk, str):\n",
    "                post_sentences.extend(sentence_split(text_chunk))\n",
    "    else:\n",
    "        post_sentences = sentence_split(post_text) if isinstance(post_text, str) else []\n",
    "    \n",
    "    sentences = pre_sentences + post_sentences\n",
    "    context_text = segment_sentences(sentences)\n",
    "    # 6. context_table\n",
    "    context_table = sample.get(\"table\")\n",
    "    # 7. reasoning\n",
    "    reasoning = len(sample[\"qa\"].get(\"steps\", [])) > 1\n",
    "    # 8. reason_type\n",
    "    steps = sample[\"qa\"].get(\"steps\", [])\n",
    "    reason_type = steps[0][\"op\"] if steps else None\n",
    "    # 9. gold_text_id\n",
    "    gold_text_id = [\"text_\" + str(i) for i in sample[\"qa\"].get(\"ann_text_rows\",[])]\n",
    "    # 10. gold_table_row\n",
    "    gold_table_row = sample[\"qa\"].get(\"ann_table_rows\", [])\n",
    "    # 11. meta\n",
    "    meta = {\n",
    "        \"tfidftopn\": sample.get(\"tfidftopn\"),\n",
    "        \"table_retrieved\": sample.get(\"table_retrieved\"),\n",
    "        \"text_retrieved\": sample.get(\"text_retrieved\"),\n",
    "    }\n",
    "\n",
    "    return {\n",
    "        \"qid\": qid,\n",
    "        \"dataset\": dataset,\n",
    "        \"question\": question,\n",
    "        \"answer\": answer,\n",
    "        \"context_text\": context_text,\n",
    "        \"context_table\": context_table,\n",
    "        \"reasoning\": reasoning,\n",
    "        \"reason_type\": reason_type,\n",
    "        \"gold_text_id\": gold_text_id,\n",
    "        \"gold_table_row\": gold_table_row,\n",
    "        \"meta\": meta,\n",
    "    }\n",
    "\n",
    "# Wrapper to process a whole dataset\n",
    "def preprocess_finqa_dataset(finqa_data, tokenizer=None, max_bpe_tokens=100):\n",
    "    return [preprocess_finqa_sample(sample, tokenizer, max_bpe_tokens) for sample in finqa_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tiktoken' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# pre-process the finqa dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m finqa_processed \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_finqa_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinQA_train_data\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[71], line 96\u001b[0m, in \u001b[0;36mpreprocess_finqa_dataset\u001b[0;34m(finqa_data, tokenizer, max_bpe_tokens)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpreprocess_finqa_dataset\u001b[39m(finqa_data, tokenizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, max_bpe_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [preprocess_finqa_sample(sample, tokenizer, max_bpe_tokens) \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m finqa_data]\n",
      "Cell \u001b[0;32mIn[71], line 96\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpreprocess_finqa_dataset\u001b[39m(finqa_data, tokenizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, max_bpe_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m---> 96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mpreprocess_finqa_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_bpe_tokens\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sample \u001b[38;5;129;01min\u001b[39;00m finqa_data]\n",
      "Cell \u001b[0;32mIn[71], line 4\u001b[0m, in \u001b[0;36mpreprocess_finqa_sample\u001b[0;34m(sample, tokenizer, max_bpe_tokens)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpreprocess_finqa_sample\u001b[39m(sample, tokenizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, max_bpe_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Set up tokenizer if not provided\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m----> 4\u001b[0m         tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mtiktoken\u001b[49m\u001b[38;5;241m.\u001b[39mget_encoding(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcl100k_base\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# OpenAI's default\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Helper: sentence split (simple, can be improved)\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msentence_split\u001b[39m(text):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tiktoken' is not defined"
     ]
    }
   ],
   "source": [
    "# pre-process the finqa dataset\n",
    "finqa_processed = preprocess_finqa_dataset(finQA_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process the finqa dataset\n",
    "finqa_processed = preprocess_finqa_dataset(finQA_train_data)\n",
    "\n",
    "# Analysis of the processed data\n",
    "print(f\"Successfully processed {len(finqa_processed)} FinQA samples\")\n",
    "print(f\"Sample structure: {list(finqa_processed[0].keys())}\")\n",
    "\n",
    "# Show first processed sample\n",
    "print(\"\\nðŸ“Š First processed sample:\")\n",
    "first_processed = finqa_processed[0]\n",
    "for key, value in first_processed.items():\n",
    "    if key == 'context_text':\n",
    "        print(f\"  {key}: {len(value)} text segments\")\n",
    "        print(f\"    First segment: {value[0][:100]}...\")\n",
    "    elif key == 'context_table':\n",
    "        print(f\"  {key}: {len(value)} table rows\")\n",
    "    elif key == 'meta':\n",
    "        print(f\"  {key}: {list(value.keys())}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Dataset statistics\n",
    "reasoning_count = sum(1 for sample in finqa_processed if sample['reasoning'])\n",
    "print(f\"\\nðŸ“ˆ Dataset Statistics:\")\n",
    "print(f\"  Total samples: {len(finqa_processed)}\")\n",
    "print(f\"  Reasoning samples: {reasoning_count} ({reasoning_count/len(finqa_processed)*100:.1f}%)\")\n",
    "print(f\"  Non-reasoning samples: {len(finqa_processed) - reasoning_count}\")\n",
    "\n",
    "# Reason type distribution\n",
    "reason_types = {}\n",
    "for sample in finqa_processed:\n",
    "    reason_type = sample.get('reason_type')\n",
    "    if reason_type:\n",
    "        reason_types[reason_type] = reason_types.get(reason_type, 0) + 1\n",
    "\n",
    "print(f\"\\nðŸ” Reason Type Distribution:\")\n",
    "for reason_type, count in sorted(reason_types.items(), key=lambda x: x[1], reverse=True)[:10]:\n",
    "    print(f\"  {reason_type}: {count} samples\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
