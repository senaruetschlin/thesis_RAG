{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "111e91c5",
   "metadata": {},
   "source": [
    "# Build the Knowledge Base "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52282a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"/Users/christel/Desktop/Thesis/thesis_repo/data/data_processed/Keep/merged_dataset.json\", \"r\") as f:\n",
    "    data = [json.loads(line) for line in f if line.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc2bb22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First entry in the dataset:\n",
      "{\n",
      "  \"ID\": \"ADI/2009/page_49.pdf\",\n",
      "  \"question\": \"what is the the interest expense in 2009?\",\n",
      "  \"answer\": \"380\",\n",
      "  \"context\": \"['interest rate to a variable interest rate based on the three-month libor plus 2.05% ( 2.05 % ) ( 2.34% ( 2.34 % ) as of october 31 , 2009 ) .', 'if libor changes by 100 basis points , our annual interest expense would change by $ 3.8 million .', 'foreign currency exposure as more fully described in note 2i .', 'in the notes to consolidated financial statements contained in item 8 of this annual report on form 10-k , we regularly hedge our non-u.s .', 'dollar-based exposures by entering into forward foreign currency exchange contracts .', 'the terms of these contracts are for periods matching the duration of the underlying exposure and generally range from one month to twelve months .', 'currently , our largest foreign currency exposure is the euro , primarily because our european operations have the highest proportion of our local currency denominated expenses .', 'relative to foreign currency exposures existing at october 31 , 2009 and november 1 , 2008 , a 10% ( 10 % ) unfavorable movement in foreign currency exchange rates over the course of the year would not expose us to significant losses in earnings or cash flows because we hedge a high proportion of our year-end exposures against fluctuations in foreign currency exchange rates .', 'the market risk associated with our derivative instruments results from currency exchange rate or interest rate movements that are expected to offset the market risk of the underlying transactions , assets and liabilities being hedged .', 'the counterparties to the agreements relating to our foreign exchange instruments consist of a number of major international financial institutions with high credit ratings .', 'we do not believe that there is significant risk of nonperformance by these counterparties because we continually monitor the credit ratings of such counterparties .', 'while the contract or notional amounts of derivative financial instruments provide one measure of the volume of these transactions , they do not represent the amount of our exposure to credit risk .', 'the amounts potentially subject to credit risk ( arising from the possible inability of counterparties to meet the terms of their contracts ) are generally limited to the amounts , if any , by which the counterparties 2019 obligations under the contracts exceed our obligations to the counterparties .', 'the following table illustrates the effect that a 10% ( 10 % ) unfavorable or favorable movement in foreign currency exchange rates , relative to the u.s .', 'dollar , would have on the fair value of our forward exchange contracts as of october 31 , 2009 and november 1 , 2008: .']\\n\\toctober 31 2009\\tnovember 1 2008\\nfair value of forward exchange contracts asset ( liability )\\t$ 6427\\t$ -23158 ( 23158 )\\nfair value of forward exchange contracts after a 10% ( 10 % ) unfavorable movement in foreign currency exchange rates asset ( liability )\\t$ 20132\\t$ -9457 ( 9457 )\\nfair value of forward exchange contracts after a 10% ( 10 % ) favorable movement in foreign currency exchange rates liability\\t$ -6781 ( 6781 )\\t$ -38294 ( 38294 )\\n['fair value of forward exchange contracts after a 10% ( 10 % ) unfavorable movement in foreign currency exchange rates asset ( liability ) .', '.', '.', '.', '.', '.', '.', '.', '.', '$ 20132 $ ( 9457 ) fair value of forward exchange contracts after a 10% ( 10 % ) favorable movement in foreign currency exchange rates liability .', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '$ ( 6781 ) $ ( 38294 ) the calculation assumes that each exchange rate would change in the same direction relative to the u.s .', 'dollar .', 'in addition to the direct effects of changes in exchange rates , such changes typically affect the volume of sales or the foreign currency sales price as competitors 2019 products become more or less attractive .', 'our sensitivity analysis of the effects of changes in foreign currency exchange rates does not factor in a potential change in sales levels or local currency selling prices. .']\",\n",
      "  \"gold_context\": {\n",
      "    \"text_1\": \"if libor changes by 100 basis points , our annual interest expense would change by $ 3.8 million .\"\n",
      "  },\n",
      "  \"operation\": \"divide(100, 100), divide(3.8, #0)\",\n",
      "  \"source\": \"FinQA\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if data:\n",
    "    first_entry = data[0]\n",
    "    print(\"First entry in the dataset:\")\n",
    "    print(json.dumps(first_entry, indent=2))\n",
    "else:\n",
    "    print(\"The dataset is empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b7e7dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FinQA: 6203 samples\n",
      "ConvFinQA: 5302 samples\n",
      "FinDER: 5696 samples\n"
     ]
    }
   ],
   "source": [
    "source_counts = {}\n",
    "for item in data:\n",
    "    source = item.get(\"source\")\n",
    "    if source:\n",
    "        source_counts[source] = source_counts.get(source, 0) + 1\n",
    "\n",
    "for source, count in source_counts.items():\n",
    "    print(f\"{source}: {count} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6fc9e3",
   "metadata": {},
   "source": [
    "Dedup the context: <br>\n",
    "If a sample has Finder and ConFinQA or FinQA as source keep the latter two over the first two as the ID contains meta data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1e0319",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 17201 records.\n",
      "Sample keys: ['ID', 'question', 'answer', 'context', 'gold_context', 'operation', 'source']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_jsonl(path: str):\n",
    "    \"\"\"Load a JSONL file into a list of dicts.\"\"\"\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f if line.strip()]\n",
    "\n",
    "file_path = \"/Users/christel/Desktop/Thesis/thesis_repo/data/data_processed/Keep/merged_dataset.json\"\n",
    "data = load_jsonl(file_path)\n",
    "print(f\"Loaded {len(data)} records.\")\n",
    "print(\"Sample keys:\", list(data[0].keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "356e7c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 17201\n",
      "IDs (unique incl. None): 7803\n",
      "Duplicate ID groups: 1997\n",
      "  - Exact-context groups (safe to collapse): 768\n",
      "  - CONFLICT groups (same ID, different context): 1229\n",
      "Rows removable safely (exact dups): 1361\n",
      "Conflicts saved to: id_context_conflicts.json (IDs with multiple distinct contexts).\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict, Counter\n",
    "import hashlib\n",
    "import json\n",
    "import os\n",
    "\n",
    "def norm_text(s):\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    return \" \".join(str(s).split()).strip().lower()\n",
    "\n",
    "def hash_text(s):\n",
    "    return hashlib.sha256(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "def audit_id_context_consistency(\n",
    "    data, \n",
    "    id_key=\"ID\", \n",
    "    context_key=\"context\", \n",
    "    sample_chars=180\n",
    "):\n",
    "    # group rows by ID\n",
    "    by_id = defaultdict(list)\n",
    "    for idx, row in enumerate(data):\n",
    "        rid = row.get(id_key)\n",
    "        by_id[rid].append((idx, row))\n",
    "\n",
    "    exact_dup_groups = 0\n",
    "    conflict_groups = 0\n",
    "    removable_row_indices = []  # rows we *could* remove because context matches exactly\n",
    "    conflicts_detail = {}       # {ID: {hash: {\"indices\":[...], \"sample\": \"...\", \"count\": n}}}\n",
    "\n",
    "    for rid, rows in by_id.items():\n",
    "        if rid is None or len(rows) == 1:\n",
    "            continue  # unique or missing id, skip\n",
    "\n",
    "        # bucket by normalized context hash\n",
    "        buckets = defaultdict(list)\n",
    "        rep = {}\n",
    "        for idx, row in rows:\n",
    "            ctx_norm = norm_text(row.get(context_key, \"\"))\n",
    "            h = hash_text(ctx_norm)\n",
    "            buckets[h].append(idx)\n",
    "            if h not in rep:\n",
    "                rep[h] = ctx_norm[:sample_chars]\n",
    "\n",
    "        if len(buckets) == 1:\n",
    "            # all contexts identical -> safe to dedupe this ID group\n",
    "            exact_dup_groups += 1\n",
    "            # mark all but the first as removable\n",
    "            all_idxs = next(iter(buckets.values()))\n",
    "            removable_row_indices.extend(sorted(all_idxs)[1:])\n",
    "        else:\n",
    "            # same ID, different context -> conflict\n",
    "            conflict_groups += 1\n",
    "            conflicts_detail[str(rid)] = {\n",
    "                h: {\n",
    "                    \"count\": len(idxs),\n",
    "                    \"indices\": sorted(idxs),\n",
    "                    \"sample\": rep[h]\n",
    "                } for h, idxs in buckets.items()\n",
    "            }\n",
    "\n",
    "    stats = {\n",
    "        \"total_rows\": len(data),\n",
    "        \"total_ids\": len(by_id),\n",
    "        \"duplicate_id_groups\": sum(1 for rid, rows in by_id.items() if rid is not None and len(rows) > 1),\n",
    "        \"exact_context_dup_groups\": exact_dup_groups,\n",
    "        \"conflict_groups\": conflict_groups,\n",
    "        \"removable_rows_count\": len(removable_row_indices),\n",
    "    }\n",
    "\n",
    "    print(\n",
    "        f\"Rows: {stats['total_rows']}\\n\"\n",
    "        f\"IDs (unique incl. None): {stats['total_ids']}\\n\"\n",
    "        f\"Duplicate ID groups: {stats['duplicate_id_groups']}\\n\"\n",
    "        f\"  - Exact-context groups (safe to collapse): {stats['exact_context_dup_groups']}\\n\"\n",
    "        f\"  - CONFLICT groups (same ID, different context): {stats['conflict_groups']}\\n\"\n",
    "        f\"Rows removable safely (exact dups): {stats['removable_rows_count']}\"\n",
    "    )\n",
    "    return stats, removable_row_indices, conflicts_detail\n",
    "\n",
    "\n",
    "stats, removable_row_indices, conflicts = audit_id_context_consistency(\n",
    "    data,\n",
    "    id_key=\"ID\",\n",
    "    context_key=\"context\"\n",
    ")\n",
    "\n",
    "\n",
    "conflicts_path = \"id_context_conflicts.json\"\n",
    "with open(conflicts_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(conflicts, f, ensure_ascii=False, indent=2)\n",
    "print(f\"Conflicts saved to: {conflicts_path} (IDs with multiple distinct contexts).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcceb988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safely removed 1361 rows (only exact duplicates by ID+context).\n",
      "Remaining rows: 15840\n",
      "Cleaned dataset written to: dataset_clean_exact_id_context.jsonl\n"
     ]
    }
   ],
   "source": [
    "def dedupe_only_when_context_matches(\n",
    "    data, \n",
    "    id_key=\"ID\", \n",
    "    context_key=\"context\", \n",
    "    keep=\"first\"  # or \"last\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Remove duplicates *only* when rows share the same ID *and* their normalized context is identical.\n",
    "    If same ID maps to multiple distinct contexts, keep them all (flag separately).\n",
    "    \"\"\"\n",
    "    # First pass: compute buckets for each ID\n",
    "    by_id = defaultdict(list)\n",
    "    for idx, row in enumerate(data):\n",
    "        rid = row.get(id_key)\n",
    "        ctx_norm = norm_text(row.get(context_key, \"\"))\n",
    "        h = hash_text(ctx_norm)\n",
    "        by_id[rid].append((idx, h))\n",
    "\n",
    "    # Decide which indices to keep\n",
    "    keep_indices = set()\n",
    "    for rid, entries in by_id.items():\n",
    "        if rid is None:\n",
    "            # keep all rows without IDs\n",
    "            keep_indices.update(idx for idx, _ in entries)\n",
    "            continue\n",
    "\n",
    "        # group by context-hash inside this ID\n",
    "        by_hash = defaultdict(list)\n",
    "        for idx, h in entries:\n",
    "            by_hash[h].append(idx)\n",
    "\n",
    "        if len(by_hash) == 1:\n",
    "            # Exact-match group: keep only first (or last)\n",
    "            for h, idxs in by_hash.items():\n",
    "                idxs_sorted = sorted(idxs)\n",
    "                if keep == \"last\":\n",
    "                    keep_indices.add(idxs_sorted[-1])\n",
    "                else:\n",
    "                    keep_indices.add(idxs_sorted[0])\n",
    "        else:\n",
    "            # Conflict: multiple contexts for same ID -> keep all\n",
    "            keep_indices.update(idx for idx, _ in entries)\n",
    "\n",
    "    cleaned = [row for i, row in enumerate(data) if i in keep_indices]\n",
    "    removed = len(data) - len(cleaned)\n",
    "    return cleaned, removed\n",
    "\n",
    "cleaned_data_safe, removed_count = dedupe_only_when_context_matches(\n",
    "    data,\n",
    "    id_key=\"ID\",\n",
    "    context_key=\"context\",\n",
    "    keep=\"first\"\n",
    ")\n",
    "\n",
    "print(f\"Safely removed {removed_count} rows (only exact duplicates by ID+context).\")\n",
    "print(f\"Remaining rows: {len(cleaned_data_safe)}\")\n",
    "\n",
    "out_path = \"dataset_clean_exact_id_context.jsonl\"\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for row in cleaned_data_safe:\n",
    "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Cleaned dataset written to: {out_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fed88fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 17201\n",
      "Unique IDs: 7803\n",
      "Duplicate IDs: 1997\n",
      "Rows with missing IDs: 0\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def check_duplicate_ids(data, id_key=\"ID\"):\n",
    "    \"\"\"Check if IDs are unique in the dataset.\"\"\"\n",
    "    seen = set()\n",
    "    duplicates = defaultdict(list)\n",
    "    missing = []\n",
    "\n",
    "    for idx, row in enumerate(data):\n",
    "        rid = row.get(id_key)\n",
    "        if rid is None:\n",
    "            missing.append(idx)\n",
    "            continue\n",
    "        if rid in seen:\n",
    "            duplicates[rid].append(idx)\n",
    "        else:\n",
    "            seen.add(rid)\n",
    "    \n",
    "    print(f\"Total records: {len(data)}\")\n",
    "    print(f\"Unique IDs: {len(seen)}\")\n",
    "    print(f\"Duplicate IDs: {len(duplicates)}\")\n",
    "    print(f\"Rows with missing IDs: {len(missing)}\")\n",
    "    return duplicates, missing\n",
    "\n",
    "duplicates, missing = check_duplicate_ids(data, id_key=\"ID\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff8ddcb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safely removed 1361 rows (only exact duplicates by ID+context).\n",
      "Remaining rows: 15840\n",
      "Cleaned dataset written to: dataset_clean_exact_id_context.jsonl\n"
     ]
    }
   ],
   "source": [
    "cleaned_data_safe, removed_count = dedupe_only_when_context_matches(\n",
    "    data,                      \n",
    "    id_key=\"ID\",               \n",
    "    context_key=\"context\",     \n",
    "    keep=\"first\"               \n",
    ")\n",
    "\n",
    "print(f\"Safely removed {removed_count} rows (only exact duplicates by ID+context).\")\n",
    "print(f\"Remaining rows: {len(cleaned_data_safe)}\")\n",
    "\n",
    "output_path = \"dataset_clean_exact_id_context.jsonl\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for row in cleaned_data_safe:\n",
    "        f.write(json.dumps(row, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Cleaned dataset written to: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccb6d2b",
   "metadata": {},
   "source": [
    "Check if all contexts are still in there after deduplicating: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "67382494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original unique contexts: 8925\n",
      "Cleaned unique contexts: 8925\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import hashlib\n",
    "\n",
    "def normalize_context(text):\n",
    "    \"\"\"Lowercase, trim spaces, collapse whitespace for fair comparison.\"\"\"\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    return \" \".join(str(text).split()).strip().lower()\n",
    "\n",
    "def hash_context(text):\n",
    "    \"\"\"Hash for memory-efficient comparison.\"\"\"\n",
    "    return hashlib.sha256(normalize_context(text).encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "# Hash contexts for original dataset\n",
    "original_contexts = {hash_context(row.get(\"context\")) for row in data}\n",
    "\n",
    "# Hash contexts for cleaned dataset\n",
    "cleaned_contexts = {hash_context(row.get(\"context\")) for row in cleaned_data_safe}\n",
    "\n",
    "print(f\"Original unique contexts: {len(original_contexts)}\")\n",
    "print(f\"Cleaned unique contexts: {len(cleaned_contexts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b10e1da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"['interest rate to a variable interest rate based on the three-month libor plus 2.05% ( 2.05 % ) ( 2.34% ( 2.34 % ) as of october 31 , 2009 ) .', 'if libor changes by 100 basis points , our annual interest expense would change by $ 3.8 million .', 'foreign currency exposure as more fully described in note 2i .', 'in the notes to consolidated financial statements contained in item 8 of this annual report on form 10-k , we regularly hedge our non-u.s .', 'dollar-based exposures by entering into forward foreign currency exchange contracts .', 'the terms of these contracts are for periods matching the duration of the underlying exposure and generally range from one month to twelve months .', 'currently , our largest foreign currency exposure is the euro , primarily because our european operations have the highest proportion of our local currency denominated expenses .', 'relative to foreign currency exposures existing at october 31 , 2009 and november 1 , 2008 , a 10% ( 10 % ) unfavorable movement in foreign currency exchange rates over the course of the year would not expose us to significant losses in earnings or cash flows because we hedge a high proportion of our year-end exposures against fluctuations in foreign currency exchange rates .', 'the market risk associated with our derivative instruments results from currency exchange rate or interest rate movements that are expected to offset the market risk of the underlying transactions , assets and liabilities being hedged .', 'the counterparties to the agreements relating to our foreign exchange instruments consist of a number of major international financial institutions with high credit ratings .', 'we do not believe that there is significant risk of nonperformance by these counterparties because we continually monitor the credit ratings of such counterparties .', 'while the contract or notional amounts of derivative financial instruments provide one measure of the volume of these transactions , they do not represent the amount of our exposure to credit risk .', 'the amounts potentially subject to credit risk ( arising from the possible inability of counterparties to meet the terms of their contracts ) are generally limited to the amounts , if any , by which the counterparties 2019 obligations under the contracts exceed our obligations to the counterparties .', 'the following table illustrates the effect that a 10% ( 10 % ) unfavorable or favorable movement in foreign currency exchange rates , relative to the u.s .', 'dollar , would have on the fair value of our forward exchange contracts as of october 31 , 2009 and november 1 , 2008: .'] october 31 2009 november 1 2008 fair value of forward exchange contracts asset ( liability ) $ 6427 $ -23158 ( 23158 ) fair value of forward exchange contracts after a 10% ( 10 % ) unfavorable movement in foreign currency exchange rates asset ( liability ) $ 20132 $ -9457 ( 9457 ) fair value of forward exchange contracts after a 10% ( 10 % ) favorable movement in foreign currency exchange rates liability $ -6781 ( 6781 ) $ -38294 ( 38294 ) ['fair value of forward exchange contracts after a 10% ( 10 % ) unfavorable movement in foreign currency exchange rates asset ( liability ) .', '.', '.', '.', '.', '.', '.', '.', '.', '$ 20132 $ ( 9457 ) fair value of forward exchange contracts after a 10% ( 10 % ) favorable movement in foreign currency exchange rates liability .', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '$ ( 6781 ) $ ( 38294 ) the calculation assumes that each exchange rate would change in the same direction relative to the u.s .', 'dollar .', 'in addition to the direct effects of changes in exchange rates , such changes typically affect the volume of sales or the foreign currency sales price as competitors 2019 products become more or less attractive .', 'our sensitivity analysis of the effects of changes in foreign currency exchange rates does not factor in a potential change in sales levels or local currency selling prices. .']\",\n",
       " \"['the following table shows annual aircraft fuel consumption and costs , including taxes , for our mainline and regional operations for 2018 , 2017 and 2016 ( gallons and aircraft fuel expense in millions ) .', 'year gallons average price per gallon aircraft fuel expense percent of total operating expenses .'] year gallons average priceper gallon aircraft fuelexpense percent of totaloperating expenses 2018 4447 $ 2.23 $ 9896 23.6% ( 23.6 % ) 2017 4352 1.73 7510 19.6% ( 19.6 % ) 2016 4347 1.42 6180 17.6% ( 17.6 % ) ['as of december 31 , 2018 , we did not have any fuel hedging contracts outstanding to hedge our fuel consumption .', 'as such , and assuming we do not enter into any future transactions to hedge our fuel consumption , we will continue to be fully exposed to fluctuations in fuel prices .', 'our current policy is not to enter into transactions to hedge our fuel consumption , although we review that policy from time to time based on market conditions and other factors .', 'fuel prices have fluctuated substantially over the past several years .', 'we cannot predict the future availability , price volatility or cost of aircraft fuel .', 'natural disasters ( including hurricanes or similar events in the u.s .', 'southeast and on the gulf coast where a significant portion of domestic refining capacity is located ) , political disruptions or wars involving oil-producing countries , economic sanctions imposed against oil-producing countries or specific industry participants , changes in fuel-related governmental policy , the strength of the u.s .', 'dollar against foreign currencies , changes in the cost to transport or store petroleum products , changes in access to petroleum product pipelines and terminals , speculation in the energy futures markets , changes in aircraft fuel production capacity , environmental concerns and other unpredictable events may result in fuel supply shortages , distribution challenges , additional fuel price volatility and cost increases in the future .', 'see part i , item 1a .', 'risk factors 2013 201cour business is very dependent on the price and availability of aircraft fuel .', 'continued periods of high volatility in fuel costs , increased fuel prices or significant disruptions in the supply of aircraft fuel could have a significant negative impact on our operating results and liquidity . 201d seasonality and other factors due to the greater demand for air travel during the summer months , revenues in the airline industry in the second and third quarters of the year tend to be greater than revenues in the first and fourth quarters of the year .', 'general economic conditions , fears of terrorism or war , fare initiatives , fluctuations in fuel prices , labor actions , weather , natural disasters , outbreaks of disease and other factors could impact this seasonal pattern .', 'therefore , our quarterly results of operations are not necessarily indicative of operating results for the entire year , and historical operating results in a quarterly or annual period are not necessarily indicative of future operating results .', 'domestic and global regulatory landscape general airlines are subject to extensive domestic and international regulatory requirements .', 'domestically , the dot and the federal aviation administration ( faa ) exercise significant regulatory authority over air carriers .', 'the dot , among other things , oversees domestic and international codeshare agreements , international route authorities , competition and consumer protection matters such as advertising , denied boarding compensation and baggage liability .', 'the antitrust division of the department of justice ( doj ) , along with the dot in certain instances , have jurisdiction over airline antitrust matters. .']\",\n",
       " '[\\'the fair value of our grants receivable is determined using a discounted cash flow model , which discounts future cash flows using an appropriate yield curve .\\', \\'as of december 28 , 2013 , and december 29 , 2012 , the carrying amount of our grants receivable was classified within other current assets and other long-term assets , as applicable .\\', \\'our long-term debt recognized at amortized cost is comprised of our senior notes and our convertible debentures .\\', \\'the fair value of our senior notes is determined using active market prices , and it is therefore classified as level 1 .\\', \\'the fair value of our convertible long-term debt is determined using discounted cash flow models with observable market inputs , and it takes into consideration variables such as interest rate changes , comparable securities , subordination discount , and credit-rating changes , and it is therefore classified as level 2 .\\', \\'the nvidia corporation ( nvidia ) cross-license agreement liability in the preceding table was incurred as a result of entering into a long-term patent cross-license agreement with nvidia in january 2011 .\\', \\'we agreed to make payments to nvidia over six years .\\', \\'as of december 28 , 2013 , and december 29 , 2012 , the carrying amount of the liability arising from the agreement was classified within other accrued liabilities and other long-term liabilities , as applicable .\\', \\'the fair value is determined using a discounted cash flow model , which discounts future cash flows using our incremental borrowing rates .\\', \\'note 5 : cash and investments cash and investments at the end of each period were as follows : ( in millions ) dec 28 , dec 29 .\\'] ( in millions ) dec 282013 dec 292012 available-for-sale investments $ 18086 $ 14001 cash 854 593 equity method investments 1038 992 loans receivable 1072 979 non-marketable cost method investments 1270 1202 reverse repurchase agreements 800 2850 trading assets 8441 5685 total cash and investments $ 31561 $ 26302 [\\'in the third quarter of 2013 , we sold our shares in clearwire corporation , which had been accounted for as available-for-sale marketable equity securities , and our interest in clearwire communications , llc ( clearwire llc ) , which had been accounted for as an equity method investment .\\', \\'in total , we received proceeds of $ 470 million on these transactions and recognized a gain of $ 439 million , which is included in gains ( losses ) on equity investments , net on the consolidated statements of income .\\', \\'proceeds received and gains recognized for each investment are included in the \"available-for-sale investments\" and \"equity method investments\" sections that follow .\\', \\'table of contents intel corporation notes to consolidated financial statements ( continued ) .\\']',\n",
       " '[\"entergy louisiana , llc management\\'s financial discussion and analysis net revenue 2008 compared to 2007 net revenue consists of operating revenues net of : 1 ) fuel , fuel-related expenses , and gas purchased for resale , 2 ) purchased power expenses , and 3 ) other regulatory charges .\", \\'following is an analysis of the change in net revenue comparing 2008 to 2007 .\\', \\'amount ( in millions ) .\\'] amount ( in millions ) 2007 net revenue $ 991.1 retail electric price -17.1 ( 17.1 ) purchased power capacity -12.0 ( 12.0 ) net wholesale revenue -7.4 ( 7.4 ) other 4.6 2008 net revenue $ 959.2 [\\'the retail electric price variance is primarily due to the cessation of the interim storm recovery through the formula rate plan upon the act 55 financing of storm costs and a credit passed on to customers as a result of the act 55 storm cost financing , partially offset by increases in the formula rate plan effective october 2007 .\\', \\'refer to \"hurricane rita and hurricane katrina\" and \"state and local rate regulation\" below for a discussion of the interim recovery of storm costs , the act 55 storm cost financing , and the formula rate plan filing .\\', \\'the purchased power capacity variance is due to the amortization of deferred capacity costs effective september 2007 as a result of the formula rate plan filing in may 2007 .\\', \\'purchased power capacity costs are offset in base revenues due to a base rate increase implemented to recover incremental deferred and ongoing purchased power capacity charges .\\', \\'see \"state and local rate regulation\" below for a discussion of the formula rate plan filing .\\', \\'the net wholesale revenue variance is primarily due to provisions recorded for potential rate refunds related to the treatment of interruptible load in pricing entergy system affiliate sales .\\', \\'gross operating revenue and , fuel and purchased power expenses gross operating revenues increased primarily due to an increase of $ 364.7 million in fuel cost recovery revenues due to higher fuel rates offset by decreased usage .\\', \\'the increase was partially offset by a decrease of $ 56.8 million in gross wholesale revenue due to a decrease in system agreement rough production cost equalization credits .\\', \\'fuel and purchased power expenses increased primarily due to increases in the average market prices of natural gas and purchased power , partially offset by a decrease in the recovery from customers of deferred fuel costs. .\\']',\n",
       " \"['the significant changes from december 31 , 2008 to december 31 , 2009 in level 3 assets and liabilities are due to : a net decrease in trading securities of $ 10.8 billion that was driven by : 2022 net transfers of $ 6.5 billion , due mainly to the transfer of debt 2013 securities from level 3 to level 2 due to increased liquidity and pricing transparency ; and net settlements of $ 5.8 billion , due primarily to the liquidations of 2013 subprime securities of $ 4.1 billion .', 'the change in net trading derivatives driven by : 2022 a net loss of $ 4.9 billion relating to complex derivative contracts , 2013 such as those linked to credit , equity and commodity exposures .', 'these losses include both realized and unrealized losses during 2009 and are partially offset by gains recognized in instruments that have been classified in levels 1 and 2 ; and net increase in derivative assets of $ 4.3 billion , which includes cash 2013 settlements of derivative contracts in an unrealized loss position , notably those linked to subprime exposures .', 'the decrease in level 3 investments of $ 6.9 billion primarily 2022 resulted from : a reduction of $ 5.0 billion , due mainly to paydowns on debt 2013 securities and sales of private equity investments ; the net transfer of investment securities from level 3 to level 2 2013 of $ 1.5 billion , due to increased availability of observable pricing inputs ; and net losses recognized of $ 0.4 billion due mainly to losses on non- 2013 marketable equity securities including write-downs on private equity investments .', 'the decrease in securities sold under agreements to repurchase of 2022 $ 9.1 billion is driven by a $ 8.6 billion net transfers from level 3 to level 2 as effective maturity dates on structured repos have shortened .', 'the decrease in long-term debt of $ 1.5 billion is driven mainly by 2022 $ 1.3 billion of net terminations of structured notes .', 'transfers between level 1 and level 2 of the fair value hierarchy the company did not have any significant transfers of assets or liabilities between levels 1 and 2 of the fair value hierarchy during 2010 .', 'items measured at fair value on a nonrecurring basis certain assets and liabilities are measured at fair value on a nonrecurring basis and therefore are not included in the tables above .', 'these include assets measured at cost that have been written down to fair value during the periods as a result of an impairment .', 'in addition , these assets include loans held-for-sale that are measured at locom that were recognized at fair value below cost at the end of the period .', 'the fair value of loans measured on a locom basis is determined where possible using quoted secondary-market prices .', 'such loans are generally classified as level 2 of the fair value hierarchy given the level of activity in the market and the frequency of available quotes .', 'if no such quoted price exists , the fair value of a loan is determined using quoted prices for a similar asset or assets , adjusted for the specific attributes of that loan .', 'the following table presents all loans held-for-sale that are carried at locom as of december 31 , 2010 and 2009 : in billions of dollars aggregate cost fair value level 2 level 3 .'] in billions of dollars aggregate cost fair value level 2 level 3 december 31 2010 $ 3.1 $ 2.5 $ 0.7 $ 1.8 december 31 2009 $ 2.5 $ 1.6 $ 0.3 $ 1.3 ['.']\"]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_context_list = [normalize_context(row.get(\"context\")) for row in cleaned_data_safe]\n",
    "original_context_list = [normalize_context(row.get(\"context\")) for row in data]\n",
    "\n",
    "cleaned_context_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12e4637",
   "metadata": {},
   "source": [
    "# Create a Vector DB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32d71619",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = cleaned_data_safe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "453383e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': 'ADI/2009/page_49.pdf',\n",
       " 'question': 'what is the the interest expense in 2009?',\n",
       " 'answer': '380',\n",
       " 'context': \"['interest rate to a variable interest rate based on the three-month libor plus 2.05% ( 2.05 % ) ( 2.34% ( 2.34 % ) as of october 31 , 2009 ) .', 'if libor changes by 100 basis points , our annual interest expense would change by $ 3.8 million .', 'foreign currency exposure as more fully described in note 2i .', 'in the notes to consolidated financial statements contained in item 8 of this annual report on form 10-k , we regularly hedge our non-u.s .', 'dollar-based exposures by entering into forward foreign currency exchange contracts .', 'the terms of these contracts are for periods matching the duration of the underlying exposure and generally range from one month to twelve months .', 'currently , our largest foreign currency exposure is the euro , primarily because our european operations have the highest proportion of our local currency denominated expenses .', 'relative to foreign currency exposures existing at october 31 , 2009 and november 1 , 2008 , a 10% ( 10 % ) unfavorable movement in foreign currency exchange rates over the course of the year would not expose us to significant losses in earnings or cash flows because we hedge a high proportion of our year-end exposures against fluctuations in foreign currency exchange rates .', 'the market risk associated with our derivative instruments results from currency exchange rate or interest rate movements that are expected to offset the market risk of the underlying transactions , assets and liabilities being hedged .', 'the counterparties to the agreements relating to our foreign exchange instruments consist of a number of major international financial institutions with high credit ratings .', 'we do not believe that there is significant risk of nonperformance by these counterparties because we continually monitor the credit ratings of such counterparties .', 'while the contract or notional amounts of derivative financial instruments provide one measure of the volume of these transactions , they do not represent the amount of our exposure to credit risk .', 'the amounts potentially subject to credit risk ( arising from the possible inability of counterparties to meet the terms of their contracts ) are generally limited to the amounts , if any , by which the counterparties 2019 obligations under the contracts exceed our obligations to the counterparties .', 'the following table illustrates the effect that a 10% ( 10 % ) unfavorable or favorable movement in foreign currency exchange rates , relative to the u.s .', 'dollar , would have on the fair value of our forward exchange contracts as of october 31 , 2009 and november 1 , 2008: .']\\n\\toctober 31 2009\\tnovember 1 2008\\nfair value of forward exchange contracts asset ( liability )\\t$ 6427\\t$ -23158 ( 23158 )\\nfair value of forward exchange contracts after a 10% ( 10 % ) unfavorable movement in foreign currency exchange rates asset ( liability )\\t$ 20132\\t$ -9457 ( 9457 )\\nfair value of forward exchange contracts after a 10% ( 10 % ) favorable movement in foreign currency exchange rates liability\\t$ -6781 ( 6781 )\\t$ -38294 ( 38294 )\\n['fair value of forward exchange contracts after a 10% ( 10 % ) unfavorable movement in foreign currency exchange rates asset ( liability ) .', '.', '.', '.', '.', '.', '.', '.', '.', '$ 20132 $ ( 9457 ) fair value of forward exchange contracts after a 10% ( 10 % ) favorable movement in foreign currency exchange rates liability .', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '$ ( 6781 ) $ ( 38294 ) the calculation assumes that each exchange rate would change in the same direction relative to the u.s .', 'dollar .', 'in addition to the direct effects of changes in exchange rates , such changes typically affect the volume of sales or the foreign currency sales price as competitors 2019 products become more or less attractive .', 'our sensitivity analysis of the effects of changes in foreign currency exchange rates does not factor in a potential change in sales levels or local currency selling prices. .']\",\n",
       " 'gold_context': {'text_1': 'if libor changes by 100 basis points , our annual interest expense would change by $ 3.8 million .'},\n",
       " 'operation': 'divide(100, 100), divide(3.8, #0)',\n",
       " 'source': 'FinQA'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b80e215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed counts: Counter({'convfinqa': 5302, 'finqa': 4842})\n",
      "Failed counts: Counter()\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "rows = cleaned_data_safe\n",
    "\n",
    "# Regex handles: TICKER/YEAR/page49.pdf  or page_49.pdf or page-49.pdf\n",
    "PAGE_RE = re.compile(\n",
    "    r\"(?P<ticker>[A-Za-z]{1,15})/(?P<year>(19|20)\\d{2})/page[_-]?(?P<page>\\d+)\\.pdf$\",\n",
    "    flags=re.IGNORECASE\n",
    ")\n",
    "\n",
    "def extract_ticker_year_page_from_id(id_value: str):\n",
    "    \"\"\"Parse 'TICKER/YEAR/page_12.pdf' from the ID field.\"\"\"\n",
    "    if not id_value:\n",
    "        return None, None, None\n",
    "    s = str(id_value).strip()\n",
    "\n",
    "    m = PAGE_RE.search(s)\n",
    "    if m:\n",
    "        return m.group(\"ticker\").upper(), int(m.group(\"year\")), int(m.group(\"page\"))\n",
    "\n",
    "    # Fallback: .../TICKER/YEAR/<file-with-digits>.pdf\n",
    "    parts = s.strip(\"/\").split(\"/\")\n",
    "    if len(parts) >= 3:\n",
    "        ticker = parts[-3].upper()\n",
    "        year_s = parts[-2]\n",
    "        file_part = parts[-1]\n",
    "        year = int(year_s) if year_s.isdigit() and len(year_s) == 4 else None\n",
    "\n",
    "        m2 = re.search(r\"(\\d+)\", file_part)\n",
    "        page = int(m2.group(1)) if m2 else None\n",
    "\n",
    "        if re.fullmatch(r\"[A-Za-z]{1,15}\", ticker):\n",
    "            return ticker, year, page\n",
    "\n",
    "    return None, None, None\n",
    "\n",
    "parsed_ok = Counter()\n",
    "parsed_fail = Counter()\n",
    "\n",
    "for r in rows:\n",
    "    src = (r.get(\"source\") or r.get(\"title\") or \"\").strip().lower()\n",
    "    # Only parse FinQA / ConFinQA / ConvFinQA\n",
    "    if src in {\"finqa\", \"confinqa\", \"convfinqa\"}:\n",
    "        # IMPORTANT: parse from ID, not source_id\n",
    "        t, y, p = extract_ticker_year_page_from_id(r.get(\"ID\"))\n",
    "        r[\"ticker\"], r[\"year\"], r[\"page\"] = t, y, p\n",
    "        if t is not None and y is not None and p is not None:\n",
    "            parsed_ok[src] += 1\n",
    "        else:\n",
    "            parsed_fail[src] += 1\n",
    "\n",
    "print(\"Parsed counts:\", parsed_ok)\n",
    "print(\"Failed counts:\", parsed_fail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8aac406c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': 'ADI/2009/page_49.pdf',\n",
       " 'question': 'what is the the interest expense in 2009?',\n",
       " 'answer': '380',\n",
       " 'context': \"['interest rate to a variable interest rate based on the three-month libor plus 2.05% ( 2.05 % ) ( 2.34% ( 2.34 % ) as of october 31 , 2009 ) .', 'if libor changes by 100 basis points , our annual interest expense would change by $ 3.8 million .', 'foreign currency exposure as more fully described in note 2i .', 'in the notes to consolidated financial statements contained in item 8 of this annual report on form 10-k , we regularly hedge our non-u.s .', 'dollar-based exposures by entering into forward foreign currency exchange contracts .', 'the terms of these contracts are for periods matching the duration of the underlying exposure and generally range from one month to twelve months .', 'currently , our largest foreign currency exposure is the euro , primarily because our european operations have the highest proportion of our local currency denominated expenses .', 'relative to foreign currency exposures existing at october 31 , 2009 and november 1 , 2008 , a 10% ( 10 % ) unfavorable movement in foreign currency exchange rates over the course of the year would not expose us to significant losses in earnings or cash flows because we hedge a high proportion of our year-end exposures against fluctuations in foreign currency exchange rates .', 'the market risk associated with our derivative instruments results from currency exchange rate or interest rate movements that are expected to offset the market risk of the underlying transactions , assets and liabilities being hedged .', 'the counterparties to the agreements relating to our foreign exchange instruments consist of a number of major international financial institutions with high credit ratings .', 'we do not believe that there is significant risk of nonperformance by these counterparties because we continually monitor the credit ratings of such counterparties .', 'while the contract or notional amounts of derivative financial instruments provide one measure of the volume of these transactions , they do not represent the amount of our exposure to credit risk .', 'the amounts potentially subject to credit risk ( arising from the possible inability of counterparties to meet the terms of their contracts ) are generally limited to the amounts , if any , by which the counterparties 2019 obligations under the contracts exceed our obligations to the counterparties .', 'the following table illustrates the effect that a 10% ( 10 % ) unfavorable or favorable movement in foreign currency exchange rates , relative to the u.s .', 'dollar , would have on the fair value of our forward exchange contracts as of october 31 , 2009 and november 1 , 2008: .']\\n\\toctober 31 2009\\tnovember 1 2008\\nfair value of forward exchange contracts asset ( liability )\\t$ 6427\\t$ -23158 ( 23158 )\\nfair value of forward exchange contracts after a 10% ( 10 % ) unfavorable movement in foreign currency exchange rates asset ( liability )\\t$ 20132\\t$ -9457 ( 9457 )\\nfair value of forward exchange contracts after a 10% ( 10 % ) favorable movement in foreign currency exchange rates liability\\t$ -6781 ( 6781 )\\t$ -38294 ( 38294 )\\n['fair value of forward exchange contracts after a 10% ( 10 % ) unfavorable movement in foreign currency exchange rates asset ( liability ) .', '.', '.', '.', '.', '.', '.', '.', '.', '$ 20132 $ ( 9457 ) fair value of forward exchange contracts after a 10% ( 10 % ) favorable movement in foreign currency exchange rates liability .', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '$ ( 6781 ) $ ( 38294 ) the calculation assumes that each exchange rate would change in the same direction relative to the u.s .', 'dollar .', 'in addition to the direct effects of changes in exchange rates , such changes typically affect the volume of sales or the foreign currency sales price as competitors 2019 products become more or less attractive .', 'our sensitivity analysis of the effects of changes in foreign currency exchange rates does not factor in a potential change in sales levels or local currency selling prices. .']\",\n",
       " 'gold_context': {'text_1': 'if libor changes by 100 basis points , our annual interest expense would change by $ 3.8 million .'},\n",
       " 'operation': 'divide(100, 100), divide(3.8, #0)',\n",
       " 'source': 'FinQA',\n",
       " 'ticker': 'ADI',\n",
       " 'year': 2009,\n",
       " 'page': 49}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a239c595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /Users/christel/Desktop/Thesis/thesis_repo/data/data_processed/Keep/context_with_metadata_dedup_enriched.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "out_path = \"/Users/christel/Desktop/Thesis/thesis_repo/data/data_processed/Keep/context_with_metadata_dedup_enriched.jsonl\"\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for r in rows:\n",
    "        f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Wrote: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38446bda",
   "metadata": {},
   "source": [
    "## Chunk the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ab15257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built 15840 base docs\n"
     ]
    }
   ],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "records = rows  \n",
    "\n",
    "def build_docs(records):\n",
    "    docs = []\n",
    "    for i, r in enumerate(records):\n",
    "        text = str(r.get(\"context\") or \"\")\n",
    "        meta = {\n",
    "            \"row_index\": i,\n",
    "            \"ID\": r.get(\"ID\"),\n",
    "            \"source\": r.get(\"source\") or r.get(\"title\"),\n",
    "            \"source_id\": r.get(\"source_id\") or r.get(\"ID\"),\n",
    "            \"ticker\": r.get(\"ticker\"),\n",
    "            \"year\": r.get(\"year\"),\n",
    "            \"page\": r.get(\"page\"),\n",
    "        }\n",
    "        docs.append(Document(page_content=text, metadata=meta))\n",
    "    return docs\n",
    "\n",
    "docs_raw = build_docs(records)\n",
    "print(f\"Built {len(docs_raw)} base docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f894aa8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "def chunk_documents_recursive(docs: list[Document], chunk_size=1500, chunk_overlap=200) -> list[Document]:\n",
    "    \"\"\"\n",
    "    Split documents into chunks of approximately `chunk_size` characters\n",
    "    with an overlap of `chunk_overlap` characters.\n",
    "    Uses a recursive splitter to handle texts without natural separators.\n",
    "    \"\"\"\n",
    "    # Create the recursive splitter with fallback separators\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "\n",
    "    chunked = []\n",
    "    for doc in docs:\n",
    "        # Copy metadata from the original doc\n",
    "        base_meta = doc.metadata.copy()\n",
    "        row_index = base_meta.get(\"row_index\")\n",
    "\n",
    "        # Split the current doc\n",
    "        pieces = splitter.split_documents([doc])\n",
    "\n",
    "        # Attach metadata to each chunk, including a unique chunk ID\n",
    "        for local_idx, chunk in enumerate(pieces):\n",
    "            chunk.metadata = {\n",
    "                **base_meta,\n",
    "                \"chunk_id\": f\"row_{row_index}_chunk_{local_idx}\"\n",
    "            }\n",
    "            chunked.append(chunk)\n",
    "\n",
    "    return chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3deadcef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 56325 chunks.\n"
     ]
    }
   ],
   "source": [
    "chunked_docs = chunk_documents_recursive(\n",
    "    docs_raw, \n",
    "    chunk_size=1500, \n",
    "    chunk_overlap=200\n",
    ")\n",
    "\n",
    "print(f\"Created {len(chunked_docs)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7b05ddf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'row_index': 0,\n",
       "  'ID': 'ADI/2009/page_49.pdf',\n",
       "  'source': 'FinQA',\n",
       "  'source_id': 'ADI/2009/page_49.pdf',\n",
       "  'ticker': 'ADI',\n",
       "  'year': 2009,\n",
       "  'page': 49,\n",
       "  'chunk_id': 'row_0_chunk_0'},\n",
       " \"['interest rate to a variable interest rate based on the three-month libor plus 2.05% ( 2.05 % ) ( 2.34% ( 2.34 % ) as of october 31 , 2009 ) .', 'if libor chan\")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunked_docs[0].metadata, chunked_docs[0].page_content[:160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ec9acbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max chunk size: 1500\n",
      "Average chunk size: 1139\n"
     ]
    }
   ],
   "source": [
    "sizes = [len(doc.page_content) for doc in chunked_docs]\n",
    "print(f\"Max chunk size: {max(sizes)}\")\n",
    "print(f\"Average chunk size: {sum(sizes)//len(sizes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33627834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks per source:\n",
      " - FinQA: 4842\n",
      " - ConvFinQA: 5302\n",
      " - FinDER: 5696\n"
     ]
    }
   ],
   "source": [
    "# count the number of chunks per source\n",
    "source_counts = {}\n",
    "for rec in records:\n",
    "    source = rec.get(\"source\")\n",
    "    if source:\n",
    "        source_counts[source] = source_counts.get(source, 0) + 1\n",
    "\n",
    "print(\"Chunks per source:\")\n",
    "for source, count in source_counts.items():\n",
    "    print(f\" - {source}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e9e72c",
   "metadata": {},
   "source": [
    "## Embed Chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02c58381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 56325 chunks for embedding\n",
      "Counts per source (input): {'finqa': 19576, 'convfinqa': 20762, 'finder': 15987}\n",
      "\n",
      "Embedding in progress...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Embedding batches: 100%|██████████| 221/221 [13:22<00:00,  3.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding dimension: 1536\n",
      "Embedded chunks saved to: /Users/christel/Desktop/Thesis/thesis_repo/data/data_processed/Keep/embedded_chunks2.json\n",
      "Non-empty text counts per source: {'finqa': 19576, 'convfinqa': 20762, 'finder': 15987}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "try:\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "except ImportError:\n",
    "    from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "# --- 1) Initialize embedder ---\n",
    "embedder = OpenAIEmbeddings(\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model=\"text-embedding-ada-002\",  # consider \"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "# --- 2) Prepare data ---\n",
    "records = []\n",
    "texts = []\n",
    "\n",
    "def clean_text(s):\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    return \" \".join(str(s).split())\n",
    "\n",
    "for doc in chunked_docs:\n",
    "    meta = doc.metadata or {}\n",
    "    text = clean_text(doc.page_content)\n",
    "    records.append({\n",
    "        \"chunk_id\":  meta.get(\"chunk_id\"),\n",
    "        \"row_index\": meta.get(\"row_index\"),\n",
    "        \"source\":    meta.get(\"source\"),\n",
    "        \"ID\":        meta.get(\"ID\"),\n",
    "        \"source_id\": meta.get(\"source_id\"),\n",
    "        \"ticker\":    meta.get(\"ticker\"),\n",
    "        \"year\":      meta.get(\"year\"),\n",
    "        \"page\":      meta.get(\"page\"),\n",
    "        \"text\":      text,\n",
    "        \"empty_text\": (len(text) == 0),\n",
    "    })\n",
    "    texts.append(text)\n",
    "\n",
    "print(f\"Prepared {len(records)} chunks for embedding\")\n",
    "\n",
    "# --- 3) Quick source count ---\n",
    "src_counts = Counter((r.get(\"source\") or \"<missing>\").strip().lower() for r in records)\n",
    "print(\"Counts per source (input):\", dict(src_counts))\n",
    "\n",
    "# --- 4) Embed in batches with progress bar ---\n",
    "BATCH_SIZE = 256\n",
    "embeddings = []\n",
    "\n",
    "print(\"\\nEmbedding in progress...\")\n",
    "for start in tqdm(range(0, len(texts), BATCH_SIZE), desc=\"Embedding batches\"):\n",
    "    batch = texts[start:start + BATCH_SIZE]\n",
    "    vecs = embedder.embed_documents(batch)\n",
    "    if len(vecs) != len(batch):\n",
    "        raise RuntimeError(f\"Embed size mismatch at batch starting {start}\")\n",
    "    embeddings.extend(vecs)\n",
    "\n",
    "# --- 5) Validation and attach embeddings ---\n",
    "if len(embeddings) != len(records):\n",
    "    raise RuntimeError(f\"Embedding count mismatch: {len(embeddings)} vs {len(records)}\")\n",
    "\n",
    "dim = None\n",
    "nan_count = 0\n",
    "for rec, emb in zip(records, embeddings):\n",
    "    if dim is None:\n",
    "        dim = len(emb)\n",
    "    if len(emb) != dim:\n",
    "        raise ValueError(f\"Inconsistent embedding dims: expected {dim}, got {len(emb)}\")\n",
    "    if any((v is None) or (isinstance(v, float) and math.isnan(v)) for v in emb):\n",
    "        nan_count += 1\n",
    "    rec[\"embedding\"] = emb\n",
    "\n",
    "if nan_count:\n",
    "    raise ValueError(f\"Found {nan_count} embeddings containing NaNs/None.\")\n",
    "\n",
    "print(f\"Embedding dimension: {dim}\")\n",
    "\n",
    "# --- 6) Save to JSON ---\n",
    "output_path = \"/Users/christel/Desktop/Thesis/thesis_repo/data/data_processed/Keep/embedded_chunks2.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(records, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Embedded chunks saved to: {output_path}\")\n",
    "\n",
    "# --- 7) Optional sanity: count non-empty text per source ---\n",
    "non_empty_counts = Counter(\n",
    "    (r.get(\"source\") or \"<missing>\").strip().lower()\n",
    "    for r in records if not r[\"empty_text\"]\n",
    ")\n",
    "print(\"Non-empty text counts per source:\", dict(non_empty_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4c2bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 56325\n",
      "\n",
      "Keys in first record: ['chunk_id', 'row_index', 'source', 'ID', 'source_id', 'ticker', 'year', 'page', 'text', 'empty_text', 'embedding'] \n",
      "\n",
      "{'ID': 'ADI/2009/page_49.pdf',\n",
      " 'chunk_id': 'row_0_chunk_0',\n",
      " 'embedding': '<1536-dim vector; head=[-0.02362525276839733, '\n",
      "              '-0.011167091317474842, 0.0005769053823314607, '\n",
      "              '-0.023784972727298737, -0.004731705877929926]>',\n",
      " 'empty_text': False,\n",
      " 'page': 49,\n",
      " 'row_index': 0,\n",
      " 'source': 'FinQA',\n",
      " 'source_id': 'ADI/2009/page_49.pdf',\n",
      " 'text': \"['interest rate to a variable interest rate based on the three-month \"\n",
      "         'libor plus 2.05% ( 2.05 % ) ( 2.34% ( 2.34 % ) as of october 31 , '\n",
      "         \"2009 ) .', 'if libor changes by 100 basis points , our annual \"\n",
      "         \"interest expense would change by $ 3.8 million .', 'foreign currency \"\n",
      "         \"exposure as more fully described in note 2i .', 'in the notes to \"\n",
      "         'consolidated financial statements contained in item 8 of this annual '\n",
      "         \"report on form 10-k , we regularly hedge our non-u.s .', \"\n",
      "         \"'dollar-based exposures by entering into forward foreign currency \"\n",
      "         \"exchange contracts .', 'the terms of these contracts are for periods \"\n",
      "         'matching the duration of the underlying exposure and generally range '\n",
      "         \"from one month to twelve months .', 'currently , our largest foreign \"\n",
      "         'currency exposure is the euro , primarily because our european '\n",
      "         'operations have the highest proportion of our local currency '\n",
      "         \"denominated expenses .', 'relative to foreign currency exposures \"\n",
      "         'existing at october 31 , 2009 and november 1 , 2008 , a 10% ( 10 % ) '\n",
      "         'unfavorable movement in foreign currency exchange rates over the '\n",
      "         'course of the year would not expose us to significant losses in '\n",
      "         'earnings or cash flows because we hedge a high proportion of our '\n",
      "         'year-end exposures against fluctuations in foreign currency exchange '\n",
      "         \"rates .', 'the market risk associated with our derivative \"\n",
      "         'instruments results from currency exchange rate or interest rate '\n",
      "         'movements that are expected to offset the market risk of the '\n",
      "         \"underlying transactions , assets and liabilities being hedged .',\",\n",
      " 'ticker': 'ADI',\n",
      " 'year': 2009}\n"
     ]
    }
   ],
   "source": [
    "# check the first sample in /Users/christel/Desktop/Thesis/thesis_repo/data/data_processed/Keep/embedded_chunks2.json\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# Path to your embeddings file\n",
    "EMBEDDED_JSON = \"/Users/christel/Desktop/Thesis/thesis_repo/data/data_processed/Keep/embedded_chunks2.json\"\n",
    "\n",
    "# Load data\n",
    "with open(EMBEDDED_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "print(f\"Total records: {len(data)}\\n\")\n",
    "\n",
    "# Take the first sample\n",
    "first = data[0]\n",
    "\n",
    "# Preview the keys\n",
    "print(\"Keys in first record:\", list(first.keys()), \"\\n\")\n",
    "\n",
    "# Pretty-print key values, truncating long fields like embeddings\n",
    "preview = first.copy()\n",
    "if \"embedding\" in preview:\n",
    "    preview[\"embedding\"] = f\"<{len(preview['embedding'])}-dim vector; head={preview['embedding'][:5]}>\"\n",
    "\n",
    "pprint(preview)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e400cda6",
   "metadata": {},
   "source": [
    "## Build FAISS Index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87b30df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts BEFORE dedupe: {'finqa': 19576, 'convfinqa': 20762, 'finder': 15987}\n",
      "Counts AFTER  dedupe: {'finqa': 19576, 'convfinqa': 20762, 'finder': 15987}\n",
      "Saved index -> /Users/christel/Desktop/Thesis/thesis_repo/data/data_processed/faiss_index.idx (ntotal=56325)\n",
      "Saved aligned meta -> /Users/christel/Desktop/Thesis/thesis_repo/data/data_processed/retriever_metadata.pkl (len=56325)\n"
     ]
    }
   ],
   "source": [
    "import os, json, faiss, numpy as np\n",
    "from collections import OrderedDict, Counter\n",
    "import math\n",
    "\n",
    "EMBEDDED_JSON = \"/Users/christel/Desktop/Thesis/thesis_repo/Can_be_deleted/Die_wahrheit/embedded_chunks2.json\"\n",
    "OUT_DIR       = \"/Users/christel/Desktop/Thesis/thesis_repo/data/data_processed/\"\n",
    "INDEX_PATH    = os.path.join(OUT_DIR, \"faiss_index.idx\")\n",
    "META_PATH     = os.path.join(OUT_DIR, \"retriever_metadata.pkl\")\n",
    "\n",
    "with open(EMBEDDED_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    recs = json.load(f)\n",
    "\n",
    "def _norm(s):\n",
    "    return \" \".join((s or \"\").split())\n",
    "\n",
    "# ---- sanity: required fields present? ----\n",
    "missing_src = sum(1 for r in recs if not r.get(\"source\"))\n",
    "missing_id  = sum(1 for r in recs if not r.get(\"ID\"))\n",
    "missing_emb = sum(1 for r in recs if \"embedding\" not in r)\n",
    "if missing_src or missing_id or missing_emb:\n",
    "    raise ValueError(\n",
    "        f\"Refusing to build: missing source={missing_src}, missing ID={missing_id}, \"\n",
    "        f\"missing embedding field={missing_emb}\"\n",
    "    )\n",
    "\n",
    "# ---- counts before any dedupe ----\n",
    "before_cnt = Counter((r.get(\"source\") or \"<missing>\").strip().lower() for r in recs)\n",
    "print(\"Counts BEFORE dedupe:\", dict(before_cnt))\n",
    "\n",
    "# ---- safer dedupe: prefer (ID, chunk_id), then (ID, text), then (source, text) ----\n",
    "dedup = OrderedDict()\n",
    "for r in recs:\n",
    "    id_ = _norm(r.get(\"ID\"))\n",
    "    cid = _norm(r.get(\"chunk_id\"))\n",
    "    src = (r.get(\"source\") or \"\").strip().lower()\n",
    "    txt = _norm(r.get(\"text\", \"\"))\n",
    "\n",
    "    if id_ and cid:\n",
    "        key = (\"id_chunk\", id_, cid)\n",
    "    elif id_:\n",
    "        key = (\"id_text\", id_, txt)\n",
    "    else:\n",
    "        key = (\"src_text\", src, txt)\n",
    "\n",
    "    if key not in dedup:\n",
    "        dedup[key] = r\n",
    "\n",
    "recs = list(dedup.values())\n",
    "\n",
    "# ---- counts after dedupe ----\n",
    "after_cnt = Counter((r.get(\"source\") or \"<missing>\").strip().lower() for r in recs)\n",
    "print(\"Counts AFTER  dedupe:\", dict(after_cnt))\n",
    "\n",
    "# ---- build matrix (float32), basic validation ----\n",
    "X = np.asarray([r[\"embedding\"] for r in recs], dtype=\"float32\")\n",
    "if X.ndim != 2:\n",
    "    raise ValueError(f\"Embedding array shape is {X.shape}, expected 2-D [n, d]\")\n",
    "if not np.isfinite(X).all():\n",
    "    bad = np.isnan(X).sum() + np.isinf(X).sum()\n",
    "    raise ValueError(f\"Found {bad} non-finite values in embeddings\")\n",
    "\n",
    "# Cosine via inner product => L2-normalize rows\n",
    "faiss.normalize_L2(X)\n",
    "\n",
    "# ---- aligned metadata (row order == FAISS row order) ----\n",
    "meta = [{\n",
    "    \"chunk_id\": r.get(\"chunk_id\"),\n",
    "    \"row_index\": r.get(\"row_index\"),\n",
    "    \"text\": r.get(\"text\", \"\"),\n",
    "    \"ID\": r.get(\"ID\"),\n",
    "    \"source\": r.get(\"source\"),\n",
    "    \"page\": r.get(\"page\"),\n",
    "    \"year\": r.get(\"year\"),\n",
    "    \"ticker\": r.get(\"ticker\"),\n",
    "} for r in recs]\n",
    "\n",
    "# ---- build exact IP index ----\n",
    "d = X.shape[1]\n",
    "index = faiss.IndexFlatIP(d)\n",
    "index.add(X)\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "faiss.write_index(index, INDEX_PATH)\n",
    "with open(META_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved index -> {INDEX_PATH} (ntotal={index.ntotal})\")\n",
    "print(f\"Saved aligned meta -> {META_PATH} (len={len(meta)})\")\n",
    "assert index.ntotal == len(meta), \"FAISS rows and metadata length must match\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935b1b77",
   "metadata": {},
   "source": [
    "# Implement the Retriever "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "acb6c9fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/rag311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(\"/Users/christel/Desktop/Thesis/thesis_repo\")\n",
    "\n",
    "\n",
    "from src.retrievers.vectorrag.index_faiss import build_faiss_index_from_json\n",
    "from src.retrievers.vectorrag.document_loader import load_json_documents\n",
    "from src.retrievers.vectorrag.chunker import chunk_documents\n",
    "from src.retrievers.vectorrag.embedder import init_embedder\n",
    "from src.retrievers.vectorrag.index_faiss import build_faiss_index, load_faiss_index\n",
    "from src.retrievers.vectorrag.retriever import rerank_search, faiss_search\n",
    "from src.retrievers.vectorrag.reranker import CrossEncoderReranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38520dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "import faiss\n",
    "import os\n",
    "\n",
    "# ---- Paths ----\n",
    "INDEX_PATH = \"/Users/christel/Desktop/Thesis/thesis_repo/data/data_processed/faiss_index.idx\"\n",
    "META_PATH  = \"/Users/christel/Desktop/Thesis/thesis_repo/data/data_processed/retriever_metadata.pkl\"\n",
    "\n",
    "# ---- Load FAISS index ----\n",
    "faiss_index = faiss.read_index(INDEX_PATH)\n",
    "\n",
    "# ---- Load metadata from pickle ----\n",
    "with open(META_PATH, \"rb\") as f:  # <-- rb (read-binary) mode for pickle\n",
    "    aligned_meta = pickle.load(f)\n",
    "\n",
    "# ---- Load embedder ----\n",
    "try:\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "except ImportError:\n",
    "    from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embedder = OpenAIEmbeddings(\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model=\"text-embedding-ada-002\"  # Or \"text-embedding-3-small\" for newer models\n",
    ")\n",
    "\n",
    "# ---- Utility to embed query ----\n",
    "def make_query_tensor(query: str) -> torch.Tensor:\n",
    "    vec = np.array(embedder.embed_query(query), dtype=\"float32\").reshape(1, -1)\n",
    "    faiss.normalize_L2(vec)\n",
    "    return torch.from_numpy(vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9001e985",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_faiss_raw(query: str, top_k: int = 5):\n",
    "    query_tensor = make_query_tensor(query)\n",
    "    indices, scores = faiss_search(faiss_index, query_tensor, top_k=top_k)\n",
    "    \n",
    "    results = []\n",
    "    for rank, (idx, score) in enumerate(zip(indices, scores), 1):\n",
    "        m = aligned_meta[int(idx)]  # metadata aligned to index\n",
    "        results.append({\n",
    "            \"rank\": rank,\n",
    "            \"score\": float(score),\n",
    "            \"chunk_id\": m.get(\"chunk_id\"),\n",
    "            \"ID\": m.get(\"ID\"),\n",
    "            \"source\": m.get(\"source\"),\n",
    "            \"text\": m.get(\"text\", \"\"),\n",
    "            \"row_index\": m.get(\"row_index\"),\n",
    "            \"page\": m.get(\"page\"),\n",
    "            \"ticker\": m.get(\"ticker\"),\n",
    "            \"year\": m.get(\"year\"),\n",
    "        })\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e1fb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004371d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                            question\n",
      "0  for acquired customer-related and network location intangibles , what is the expected annual amortization expenses , in millions?\n",
      "                                                                                                                                                                                                                                                                                                                                                                                                          gold_context\n",
      "0  {'text_0': 'american tower corporation and subsidiaries notes to consolidated financial statements ( 3 ) consists of customer-related intangibles of approximately $ 75.0 million and network location intangibles of approximately $ 72.7 million .', 'text_1': 'the customer-related intangibles and network location intangibles are being amortized on a straight-line basis over periods of up to 20 years .'}\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.DataFrame(json.load(open(\"/Users/christel/Desktop/Thesis/thesis_repo/data/data_processed/Keep/Train_Val_Test/df_train.json\")))\n",
    "print(train_df[[\"question\"]].head(1))\n",
    "print(train_df[[\"gold_context\"]].head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa73d717",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option(\"display.max_rows\")\n",
    "pd.reset_option(\"display.max_columns\")\n",
    "pd.reset_option(\"display.max_colwidth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67104541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top results for query: for acquired customer-related and network location intangibles , what is the expected annual amortization expenses , in millions?\n",
      "\n",
      " 1. score=0.876 | ID=None | source=None\n",
      "    ['( 1 ) consists of customer relationships of approximately $ 205.4 million and network location intangibles of approximately $ 55.5 million .', 'the customer relationships and network location intang...\n",
      "\n",
      " 2. score=0.873 | ID=None | source=None\n",
      "    92 | 2017 form 10-k finite-lived intangible assets are amortized over their estimated useful lives and tested for impairment if events or changes in circumstances indicate that the asset may be impair...\n",
      "\n",
      " 3. score=0.872 | ID=None | source=None\n",
      "    ['( 1 ) consists of customer-related intangibles of approximately $ 0.4 million and network location intangibles of approximately $ 0.7 million .', 'the customer-related intangibles and network locati...\n",
      "\n",
      " 4. score=0.870 | ID=None | source=None\n",
      "    ['american tower corporation and subsidiaries notes to consolidated financial statements ( 3 ) consists of customer-related intangibles of approximately $ 75.0 million and network location intangibles...\n",
      "\n",
      " 5. score=0.868 | ID=None | source=None\n",
      "    non-current assets\t2332 property and equipment\t26711 intangible assets ( 1 )\t21079 other non-current liabilities\t-1349 ( 1349 ) fair value of net assets acquired\t$ 57536 goodwill ( 2 )\t5998 ( 1 ) cons...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"for acquired customer-related and network location intangibles , what is the expected annual amortization expenses , in millions?\"\n",
    "results = search_faiss_raw(query, top_k=5)\n",
    "\n",
    "print(f\"\\nTop results for query: {query}\\n\")\n",
    "for r in results:\n",
    "    preview = r[\"text\"].replace(\"\\n\", \" \")[:200]\n",
    "    print(f\"{r['rank']:>2}. score={r['score']:.3f} | ID={r['ID']} | source={r['source']}\")\n",
    "    print(f\"    {preview}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78685c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "import os\n",
    "\n",
    "# ---- Make sure embedder is initialized ----\n",
    "try:\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "except ImportError:\n",
    "    from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embedder = OpenAIEmbeddings(\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model=\"text-embedding-ada-002\"\n",
    ")\n",
    "\n",
    "# ---- Define the function ----\n",
    "def embed_query_vec(query: str) -> np.ndarray:\n",
    "    \"\"\"Return normalized query vector for cosine FAISS search.\"\"\"\n",
    "    v = np.array(embedder.embed_query(query), dtype=\"float32\")[None, :]\n",
    "    faiss.normalize_L2(v)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3ab13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices: [6084, 8719, 3491, 17, 11401]\n",
      "scores:  [0.876, 0.873, 0.872, 0.87, 0.868]\n",
      "unique idxs: 5 of 5\n",
      "i=  6084 score=0.876 chunk_id=row_2118_chunk_1 id=None src=None text_hash=f1730790f4 len=1495\n",
      "i=  8719 score=0.873 chunk_id=row_5094_chunk_0 id=None src=None text_hash=2d91bb20e4 len=1326\n",
      "i=  3491 score=0.872 chunk_id=row_1040_chunk_1 id=None src=None text_hash=55974a32a9 len=1498\n",
      "i=    17 score=0.870 chunk_id=row_5_chunk_0 id=None src=None text_hash=18eef8cab3 len=1486\n",
      "i= 11401 score=0.868 chunk_id=row_7696_chunk_1 id=None src=None text_hash=f32fb15569 len=997\n"
     ]
    }
   ],
   "source": [
    "# show raw FAISS output (indices + scores) and check uniqueness\n",
    "q = query\n",
    "k = 5\n",
    "\n",
    "v = embed_query_vec(q)                 \n",
    "scores, idxs = index.search(v, k)\n",
    "\n",
    "print(\"indices:\", idxs[0].tolist())\n",
    "print(\"scores: \", [round(float(s), 3) for s in scores[0]])\n",
    "print(\"unique idxs:\", len(set(idxs[0].tolist())), \"of\", len(idxs[0]))\n",
    "\n",
    "import hashlib\n",
    "def h(s): return hashlib.sha256((s or \"\").encode(\"utf-8\")).hexdigest()[:10]\n",
    "\n",
    "for i, s in zip(idxs[0], scores[0]):\n",
    "    m = meta[int(i)]\n",
    "    print(f\"i={int(i):6d} score={float(s):.3f} chunk_id={m.get('chunk_id')} id={m.get('ID')} src={m.get('source')}\"\n",
    "          f\" text_hash={h(m.get('text',''))} len={len(m.get('text',''))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e624c583",
   "metadata": {},
   "source": [
    "Try to improve performance using a reranker: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3045db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 10 results for query: for acquired customer-related and network location intangibles , what is the expected annual amortization expenses , in millions?\n",
      "\n",
      " 1. score=0.9933  ( 1 ) consists of customer-related intangibles of approximately $ 0.4 million and network location intangibles of approximately $ 0.7 million . the customer-related intangibles and network location intangibles are being  ...\n",
      "\n",
      " 2. score=0.9933  ( 1 ) consists of customer-related intangibles of approximately $ 80.0 million and network location intangibles of approximately $ 38.0 million . the customer-related intangibles and network location intangibles are bein ...\n",
      "\n",
      " 3. score=0.9931  american tower corporation and subsidiaries notes to consolidated financial statements ( 3 ) consists of customer-related intangibles of approximately $ 75.0 million and network location intangibles of approximately $ 72 ...\n",
      "\n",
      " 4. score=0.9928  ( 1 ) consists of customer relationships of approximately $ 205.4 million and network location intangibles of approximately $ 55.5 million . the customer relationships and network location intangibles are being amortized ...\n",
      "\n",
      " 5. score=0.9922  ['american tower corporation and subsidiaries notes to consolidated financial statements ( 3 ) consists of customer-related intangibles of approximately $ 75.0 million and network location intangibles of approximately $  ...\n",
      "\n",
      " 6. score=0.9917  ['( 1 ) consists of customer-related intangibles of approximately $ 80.0 million and network location intangibles of approximately $ 38.0 million .', 'the customer-related intangibles and network location intangibles are ...\n",
      "\n",
      " 7. score=0.9911  ['( 1 ) consists of customer-related intangibles of approximately $ 0.4 million and network location intangibles of approximately $ 0.7 million .', 'the customer-related intangibles and network location intangibles are b ...\n",
      "\n",
      " 8. score=0.9889  ['( 1 ) consists of customer relationships of approximately $ 205.4 million and network location intangibles of approximately $ 55.5 million .', 'the customer relationships and network location intangibles are being amor ...\n",
      "\n",
      " 9. score=0.9707  none of the intangibles has significant residual value . we are amortizing the customer relationship intangibles over estimated useful lives ranging from 13 to 16 years based on a straight-line basis because the amortiza ...\n",
      "\n",
      "10. score=0.9701  non-current assets\t2332 property and equipment\t26711 intangible assets ( 1 )\t21079 other non-current liabilities\t-1349 ( 1349 ) fair value of net assets acquired\t$ 57536 goodwill ( 2 )\t5998 ( 1 ) consists of customer-rel ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIG ---\n",
    "TOP_K_FAISS = 50   # retrieve this many from FAISS\n",
    "RERANK_K    = 10   # keep this many after reranking\n",
    "QUERY       = q\n",
    "\n",
    "# --- imports ---\n",
    "import os, json, numpy as np, faiss, torch\n",
    "from src.retrievers.vectorrag.retriever import rerank_search    \n",
    "from src.retrievers.vectorrag.reranker import CrossEncoderReranker \n",
    "\n",
    "# Load index + metadata\n",
    "index = faiss.read_index(INDEX_PATH)\n",
    "with open(META_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    aligned_meta = json.load(f)\n",
    "\n",
    "# Build the “all_documents” list in FAISS order (must align with index add-order)\n",
    "all_documents = [m.get(\"text\", \"\") for m in aligned_meta]\n",
    "\n",
    "# ---- Build the SAME embedder used for documents ----\n",
    "try:\n",
    "    from langchain_openai import OpenAIEmbeddings\n",
    "except ImportError:\n",
    "    from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "embedder = OpenAIEmbeddings(\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "    model=\"text-embedding-ada-002\",\n",
    ")\n",
    "\n",
    "def make_query_tensor(text: str) -> torch.Tensor:\n",
    "    \"\"\"Embed + L2-normalize so inner product = cosine.\"\"\"\n",
    "    v = np.array(embedder.embed_query(text), dtype=\"float32\").reshape(1, -1)\n",
    "    faiss.normalize_L2(v)\n",
    "    return torch.from_numpy(v)\n",
    "\n",
    "# ---- Call your reranker pipeline ----\n",
    "q_emb = make_query_tensor(QUERY)\n",
    "\n",
    "reranked_texts, reranked_scores = rerank_search(\n",
    "    query=QUERY,\n",
    "    query_embedding=q_emb,\n",
    "    index=index,\n",
    "    all_documents=all_documents,\n",
    "    top_k=TOP_K_FAISS,\n",
    "    rerank_k=RERANK_K,\n",
    "    return_scores=True\n",
    ")\n",
    "\n",
    "# ---- Pretty print ----\n",
    "print(f\"\\nTop {RERANK_K} results for query: {QUERY}\\n\")\n",
    "for i, (txt, sc) in enumerate(zip(reranked_texts, reranked_scores), 1):\n",
    "    preview = (txt or \"\").replace(\"\\n\", \" \")[:220]\n",
    "    print(f\"{i:>2}. score={float(sc):.4f}  {preview} ...\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
